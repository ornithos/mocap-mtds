{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORNN using the GRU layer from Martinez/PyTorch for legs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Revise\n",
    "using LinearAlgebra, Random\n",
    "using StatsBase, Statistics\n",
    "using Distributions, MultivariateStats   # Categorical, P(P)CA\n",
    "using Quaternions    # For manipulating 3D Geometry\n",
    "using MeshCat        # For web visualisation / animation\n",
    "using PyPlot         # Plotting\n",
    "using AxUtil         # Cayley, skew matrices\n",
    "using Flux, CuArrays # Optimisation\n",
    "using DSP            # convolution / low-pass (MA) filter\n",
    "\n",
    "# small utils libraries\n",
    "using ProgressMeter, Formatting, ArgCheck, Dates\n",
    "using BSON, NPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_MOCAP_MTDS = \".\" \n",
    "\n",
    "# Data loading and transformation utils\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"io.jl\"))\n",
    "\n",
    "# MeshCat skeleton visualisation tools\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"mocap_viz.jl\"))\n",
    "\n",
    "# Data scaling utils\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"util.jl\"))\n",
    "\n",
    "# Models: LDS\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"models.jl\"))\n",
    "\n",
    "# Table visualisation\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"pretty.jl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "##    CUSTOM WIDELY USED FUNCTIONS\n",
    "function zero_grad!(P) \n",
    "    for x in P\n",
    "        x.grad .= 0\n",
    "    end\n",
    "end\n",
    "\n",
    "const NoGradModels = Union{model.MyLDS_ng, model.ORNN_ng}\n",
    "const _var_cache = IdDict()\n",
    "\n",
    "mse(Δ::AbstractArray, scale=size(Δ, 1)) = mean(x->x^2, Δ)*scale\n",
    "\n",
    "function mse(d::mocaputil.DataIterator, m::NoGradModels)\n",
    "    obj = map(d) do (y, u, new_state)\n",
    "        new_state && (m.h .= zeros(size(m, 1))) \n",
    "        u_ = (m isa model.ORNN_ng && length(m.inpnn)) > 0 ? vcat(u, m.inpnn(u)) : u\n",
    "        mse(m(u_) - y)\n",
    "    end\n",
    "    m.h .= zeros(size(m, 1))\n",
    "    return dot(obj, mocaputil.weights(d, as_pct=true))\n",
    "end\n",
    "\n",
    "\n",
    "mse(Ds::Vector{D}, m::NoGradModels) where {D <: Dict} = mse(mocaputil.DataIterator(Ds, 1000000), m)\n",
    "mse(D::Dict, m::NoGradModels) = mse(m(D[:U]) - D[:Y])\n",
    "mse(V::Tuple, m::NoGradModels) = mse(m(V[2]) - V[1])\n",
    "\n",
    "# Calculate variance\n",
    "function _calc_var!(cache::IdDict, d::mocaputil.DataIterator)\n",
    "    Y = reduce(hcat, [y for (y, u, h) in d])\n",
    "    _var_cache[d] = var(Y, dims=2)\n",
    "end\n",
    "\n",
    "function _calc_var!(cache::IdDict, d::Vector{D}) where {D <: Dict}\n",
    "    Y = reduce(hcat, [dd[:Y] for dd in d])\n",
    "    _var_cache[d] = var(Y, dims=2)\n",
    "end\n",
    "\n",
    "function Statistics.var(d::Union{mocaputil.DataIterator, Vector{D}}) where {D <: Dict}\n",
    "    !haskey(_var_cache, d) && _calc_var!(_var_cache, d)\n",
    "    return _var_cache[d]\n",
    "end\n",
    "Statistics.var(d::Dict) = var(d[:Y], dims=2)\n",
    "\n",
    "# Standardised MSE\n",
    "smse(Δ::AbstractArray, scale=size(Δ, 1)) = mse(Δ, scale) / sum(var(Δ, dims=2))\n",
    "\n",
    "smse(d::mocaputil.DataIterator, m::NoGradModels) = mse(d, m) / sum(var(d))\n",
    "smse(D::Dict, m::NoGradModels) = mse(m(D[:U]) - D[:Y]) / sum(var(D))\n",
    "smse(Ds::Vector{D}, m::NoGradModels) where {D <: Dict} = mse(mocaputil.DataIterator(Ds, 1000000), m) / sum(var(Ds))\n",
    "smse(D::Tuple, m::NoGradModels) = mse(D, m) / sum(var(D[1], dims=2))\n",
    "\n",
    "rsmse(args...) = sqrt(smse(args...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mse(d::mocaputil.DataIterator, m::model.MTLDS_ng, z::AbstractArray)\n",
    "    @argcheck size(z, 2) == length(d)\n",
    "    obj = map(enumerate(d)) do (ii, (y, u, new_state))\n",
    "        new_state && (m.h .= zeros(size(m, 1))) \n",
    "        cmodel = model.make_lds(m, z[:,ii], m.η_h)\n",
    "        mse(cmodel(u) - y)\n",
    "    end\n",
    "    m.h .= zeros(size(m, 1))\n",
    "    return dot(obj, mocaputil.weights(d, as_pct=true))\n",
    "end\n",
    "\n",
    "\n",
    "function mse(d::mocaputil.DataIterator, m::model.ORNN_ng, z::AbstractArray, nn::Chain)\n",
    "    @argcheck size(z, 2) == length(d)\n",
    "    obj = map(enumerate(d)) do (ii, (y, u, new_state))\n",
    "        new_state && (m.h .= zeros(size(m, 1))) \n",
    "        cmodel = model.make_rnn_psi(m, Tracker.data(nn(z[:,ii])), 1f0)\n",
    "        u_ = length(m.inpnn) > 0 ? vcat(u, m.inpnn(u)) : u\n",
    "        mse(cmodel(u_) - y)\n",
    "    end\n",
    "    m.h .= zeros(size(m, 1))\n",
    "    return dot(obj, mocaputil.weights(d, as_pct=true))\n",
    "end\n",
    "\n",
    "smse(d::mocaputil.DataIterator, m::model.MTLDS_ng, z::AbstractArray) = mse(d, m, z) / sum(var(d))\n",
    "smse(d::mocaputil.DataIterator, m::model.ORNN_ng, z::AbstractArray, nn::Chain) = mse(d, m, z, nn) / sum(var(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data\n",
    "See `2_Preprocess.ipynb`\n",
    "\n",
    "**Note that in the current harddisk state**,\n",
    "* `edin_Ys_30fps.bson` was created with `include_ftcontact=false, fps=30`,\n",
    "* `edin_Xs_30fps.bson` was created with `include_ftcontact=true, include_ftmid=true, joint_pos=false, fps=fps, speed=false`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task descriptors\n",
    "styles_lkp = BSON.load(\"styles_lkp\")[:styles_lkp];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "Usraw = BSON.load(\"edin_Xs_30fps.bson\")[:Xs];\n",
    "Ysraw = BSON.load(\"edin_Ys_30fps.bson\")[:Ys];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ysraw = [y[2:end,:] for y in Ysraw]\n",
    "Usraw = [hcat(u[2:end,1:end-8], u[1:end-1,end-7:end]) for u in Usraw];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise inputs and outputs\n",
    "standardize_Y = fit(mocaputil.MyStandardScaler, reduce(vcat, Ysraw),  1)\n",
    "standardize_U = fit(mocaputil.MyStandardScaler, reduce(vcat, Usraw),  1)\n",
    "\n",
    "Ys = [mocaputil.scale_transform(standardize_Y, y[2:end, :] ) for y in Ysraw];  # (1-step ahead of u)\n",
    "Us = [mocaputil.scale_transform(standardize_U, u[1:end-1,:]) for u in Usraw];  # (1-step behind y)\n",
    "\n",
    "@assert (let c=cor(Usraw[1][1:end-1, :], Ysraw[1][2:end, :], dims=1); \n",
    "        !isapprox(maximum(abs.(c[.!isnan.(c)])), 1.0); end) \"some input features perfectly correlated\"\n",
    "\n",
    "# to invert: `mocaputil.invert(standardize_Y, y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENSE CHECK\n",
    "# check that no bugs in constructing U, Y (i.e. esp that t's align and can predict U --> Y)\n",
    "let c=cor(reduce(vcat, Us) |>f64, reduce(vcat, Ys) |> f64, dims=1)\n",
    "    imshow(c, aspect=\"auto\")\n",
    "    nonan_c = c[.!isnan.(c)]\n",
    "    title(format(\"max (abs) corrcoeff: {:.8f}\", maximum(abs.(nonan_c))))\n",
    "    flush(stdout)\n",
    "#     display(findmax(reshape(nonan_c, size(c, 1) - 2, size(c,2))))\n",
    "#     printfmtln(\"10th best result {:.5f}\", reverse(sort(nonan_c))[10]) \n",
    "end\n",
    "colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "expmtdata = mocapio.ExperimentData(Ysraw, [Matrix(y') for y in Ys], \n",
    "    [Matrix(u') for u in Us], styles_lkp);\n",
    "# see ?mocapio.get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "function data_ahead(dataIters, start_ix, k_ahead)\n",
    "    reduce(hcat, [dataIters[i][1] for i in start_ix+1:start_ix+k_ahead]),\n",
    "    reduce(hcat, [dataIters[i][2] for i in start_ix+1:start_ix+k_ahead])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MT-ORNN (Hard-EM) experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training set for STL and pooled models.\n",
    "style_ix = 1\n",
    "d = d_state = 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPool, validPool, testPool = mocapio.get_data(expmtdata, style_ix, :split, :pooled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct batch iterator\n",
    "batch_size = 64\n",
    "min_size = 50\n",
    "trainIter = mocaputil.DataIterator(trainPool, batch_size, min_size=min_size);\n",
    "trainIters = collect(trainIter);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# style segment lookups\n",
    "style_names = [\"angry\", \"childlike\", \"depressed\", \"neutral\", \"old\", \"proud\", \"sexy\", \"strutting\"];\n",
    "segment_lkp = [length(mocaputil.DataIterator(mocapio.get_data(expmtdata, i, :train, :stl, split=[0.875,0.125]),\n",
    "            batch_size, min_size=50)) for i in 2:8];\n",
    "segment_lkp = [collect(i+1:j) for (i,j) in zip(vcat(0, cumsum(segment_lkp[1:end-1])), cumsum(segment_lkp))];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base model\n",
    "\n",
    "Model 1: Bottom half only: indices 1-3 (root speed: x,z,w), 4 (root height: y), 5-28 (legs, feet: x,y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru1_data = npzread(format(\"gru1legs/edin_Us_legs{:d}_30_fps.npz\", style_ix))\n",
    "new_Us = [gru1_data[string(i)] for i in 1:length(gru1_data)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "expmtdata_legs = mocapio.ExperimentData(Ysraw, [Matrix(y') for y in Ys], \n",
    "    [Matrix(u') for u in new_Us], styles_lkp);\n",
    "\n",
    "\n",
    "pool2 = mocapio.get_data(expmtdata_legs, style_ix, :split, :pooled)\n",
    "trainPool2, validPool2, testPool2 = pool2\n",
    "trainIter2, validIter2, testIter2 = map(x->mocaputil.DataIterator(x, 64, min_size=min_size), pool2)\n",
    "trainIters2, validIters2, testIters2 = map(collect, (trainIter2, validIter2, testIter2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = subplots(5,4,figsize=(10,10))\n",
    "_batch_num = 55\n",
    "offset = 0\n",
    "_Yb, _Ub = data_ahead(trainIters, _batch_num-1, 2)\n",
    "_Yb2, _Ubhat = data_ahead(trainIters2, _batch_num-1, 2)\n",
    "for i in 1:20\n",
    "    axs[:][i].plot(_Yb'[:, i+offset])\n",
    "    axs[:][i].plot(_Ubhat'[:, i+offset], alpha=0.4)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "* We **do** want the model to learn the same root/leg motion when turning various tight corners at varying speeds. This is *despite the fact that many styles contain very few corner types, and little variation in speed*. **We want generalisation over basic motion**.\n",
    "* We **do not** want the model to learn the same body position and arm motions when travelling certain trajectories and speeds. But we have the same situation that *many styles contain only a few trajectory types and little variation in speed*. **We do not want generalisation over style types**.\n",
    "\n",
    "The model cannot possibly distinguish that which we do wish to generalise and that which we do not. All of these factors will be highly entangled in whatever representation it learns. The strategy of learning a pooled model for the root/legs and a bespoke (MT) model for the rest of the body works ok, so long as the root model does not extract too many features in the legs, and hence cause information leakage about style. Unfortunately we have the situation that either the challenging corner set pieces are jerky (low model capacity) or information leakage occurs (high model capacity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MT-ORNN\n",
    "Note that the MTORNN object is still not mature, and I'm just manipulating directly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_ixs = 1:28\n",
    "train_neutral = mocapio.get_data(expmtdata, 4, :train, :stl, concat=true, simplify=true);\n",
    "train_neutral[:Y] = train_neutral[:Y][model_1_ixs, :];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init a:\n",
    "# extract cθ from a spectral LDS fit for ORNN initialisation (see Henaff et al. for block diag init motivation)\n",
    "lds_evs = cos.(rand(Float32, Int(d/2))*π/8)\n",
    "blkvals = vcat([sqrt((1-ct)/(1+ct)) for ct in real(lds_evs)]', \n",
    "                zeros(Float32, floor(Int, d_state/2))')[1:end-1]\n",
    "a = AxUtil.Math.unmake_lt_strict(diagm(-1=>blkvals), d_state)\n",
    "a = vcat(ones(Float32, 10)*atanh(0.5f0), ones(Float32, 10)*atanh(0.75f0), \n",
    "    ones(Float32, d_state-20)*atanh(0.9f0), a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out, d_in = length(model_1_ixs), size(train_neutral[:U], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = Tracker.param(Flux.glorot_normal(64, d));\n",
    "B = Tracker.param(Flux.glorot_normal(d, 28));\n",
    "D = Tracker.param(Flux.glorot_normal(64, 28));\n",
    "D.data[1:28, 1:28] = f32(Matrix(I, 28, 28))\n",
    "b_offset = Tracker.param(zeros(Float32, d));\n",
    "d_offset = Tracker.param(zeros(Float32, 64));\n",
    "hstate = Tracker.param(zeros(Float32, d))\n",
    "# initialise base model\n",
    "ornn_base = model.ORNN_g(param(a), B, b_offset, hstate, C, D, d_offset, tanh, Chain()); #inpnn);\n",
    "ornn_base_ng = model.make_nograd(ornn_base);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ornn_optim = copy(ornn_base)\n",
    "ornn_optim_ng = model.make_nograd(ornn_optim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-task manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3                 # dimension of manifold\n",
    "d_nn = 200            # \"complexity\" of manifold\n",
    "d_subspace = 30;      # dim of subspace (⊆ parameter space) containg the manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilogy(-5:0.1:1, exp.(-5:0.1:1))\n",
    "semilogy(-5:0.1:1, σ.(3 .* (-5:0.1:1)))\n",
    "semilogy(-5:0.1:1, σ.(5 .* (-5:0.1:1)))\n",
    "gca().axhline(0.01, linestyle=\":\", color=\"grey\")\n",
    "\n",
    "gca().axvline(log(0.01), linestyle=\":\", color=\"grey\")\n",
    "gca().axvline(-log(inv(0.01) -1)/3, linestyle=\":\", color=\"grey\")\n",
    "gca().axvline(-log(inv(0.01) -1)/5, linestyle=\":\", color=\"grey\")\n",
    "title(\"Choice of std parameterisation.\"); gcf().set_size_inches(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_par = [length(x) for x in model.pars_no_inpnn(ornn_optim)] |> sum\n",
    "nn = Chain(Dense(k, d_nn, tanh), Dense(d_nn, d_subspace, identity), \n",
    "    Dense(d_subspace, d_par, identity, initW = ((dims...)->Flux.glorot_uniform(dims...)*0.05f0)))\n",
    "nn_ng = mapleaves(Tracker.data, nn)\n",
    "Zmu = Flux.param(randn(Float32, k, length(trainIter))*0.01f0);\n",
    "Zlogit_s = Flux.param(-ones(Float32, k, length(trainIter))*1.76f0);  # ≈ 0.005 std isotropic posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimise_upper!(ornn_optim, nn, Zmu, Zlogit_s, pars, trainIter, opt, \n",
    "        η, n_epochs, shuffle_examples, lower_ixs)\n",
    "    opt.eta = η\n",
    "    nB = length(trainIter)\n",
    "    W = mocaputil.weights(trainIter; as_pct=false) ./ batch_size\n",
    "    history = ones(n_epochs*nB, 2) * NaN\n",
    "\n",
    "    for ee in 1:n_epochs\n",
    "        rnn = RNN(size(ornn_optim,3), size(ornn_optim,1), ornn_optim.σ)\n",
    "\n",
    "        if shuffle_examples\n",
    "            mtl_ixs, trainData = mocaputil.indexed_shuffle(trainIter)\n",
    "        else\n",
    "            mtl_ixs, trainData = 1:length(trainIter), trainIter\n",
    "        end\n",
    "        for (ii, (Yb, Ub, h0)) in zip(mtl_ixs, trainData)\n",
    "            h0 && Flux.reset!(rnn)\n",
    "            Tb = size(Yb, 2)      # not constant\n",
    "            cmu, cstd = Zmu[:, ii], σ.(3*Zlogit_s[:, ii])\n",
    "            Zs_post = cmu .+ (randn(Float32, k) .* cstd)\n",
    "            \n",
    "            # Log reconstruction term\n",
    "            c_ornn = model.make_rnn_psi(ornn_optim, nn(Zs_post), 1f0)\n",
    "            model.build_rnn!(rnn, c_ornn)\n",
    "            x̂ = reduce(hcat, [rnn(Ub[:,i]) for i in 1:Tb])  |> Tracker.collect\n",
    "            #         ŷ = let m=ornn_optim; m.C*x̂ + m.D*Ub .+ m.d; end   # keep same C, D, d ∀ tasks\n",
    "            DU = vcat(Ub, c_ornn.D[lower_ixs[end]+1:end,:]*Ub)       # strictly residual connection to lower\n",
    "            ŷ = let m=c_ornn; m.C*x̂ + DU .+ m.d; end                 # adapt C, D, d too.\n",
    "            obj = mean(x->x^2, Yb - ŷ) * 8^2 * W[ii]\n",
    "\n",
    "            # KL term\n",
    "            KLD = -0.5f0 * sum(1 .+ 2 * log.(cstd) .- cmu.^2 .- cstd.^2)\n",
    "            obj += KLD\n",
    "            \n",
    "            Tracker.back!(obj)\n",
    "            history[(ee-1)*nB + ii, :] = [obj.data - KLD.data, obj.data]\n",
    "\n",
    "            if ii % 34 == 0\n",
    "                for layer in nn.layers\n",
    "                    obj += 1e-3*sum(abs, layer.W)\n",
    "                    obj += 1e-3*sum(abs, layer.b)\n",
    "                end\n",
    "\n",
    "                for p in pars\n",
    "                    Tracker.update!(opt, p, Tracker.grad(p))\n",
    "                end\n",
    "            end\n",
    "\n",
    "            rnn.cell.h.data .= 0       # initial state is a param :/. Easier to reset here.\n",
    "            Flux.truncate!(rnn);\n",
    "        end\n",
    "        printfmtln(\"{:02d}: {:.5f} ({:.5f})\", ee, sqrt.(mean(history[(1:nB) .+ nB*(ee-1), :], dims=1))[:]...); \n",
    "        flush(stdout)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = ADAM(1e-4)\n",
    "pars = Flux.params(nn, Zmu) #, Zlogit_s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\approx hard EM initialisation\n",
    "history1 = optimise_upper!(ornn_optim, nn, Zmu, Zlogit_s, pars, trainIter2, opt, 1e-3, 200, true, 1:28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do \"global optimisation\" of latent zs per 64-length seq here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset any gradient accrued on previous step while not optimised\n",
    "Zlogit_s.grad .= 0\n",
    "\n",
    "#= rescale latent space so optimisation doesn't have to waste effort on this.\n",
    "   (Note that can take many hundreds of iterations to spread out into N(0,I),\n",
    "    and will cause damage to reconstruction while sigmas overlap each other\n",
    "    in order to get low hanging fruit of massive sigma penalty in KL.) =#\n",
    "orig_std = std(Tracker.data(Zmu), dims=2)\n",
    "Zmu = Zmu ./ orig_std;\n",
    "nn_ng.layers[1].W .*= orig_std';\n",
    "\n",
    "# include Zlogit_s now in optimisation\n",
    "pars = Flux.params(nn, Zmu, Zlogit_s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = optimise_upper!(ornn_optim, nn, Zmu, Zlogit_s, pars, trainIter2, opt, 1e-3, 200, true, 1:28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = optimise_upper!(ornn_optim, nn, Zmu, Zlogit_s, pars, trainIter2, opt, 1e-3, 100, true, 1:28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = optimise_upper!(ornn_optim, nn, Zmu, Zlogit_s, pars, trainIter2, opt, 7e-4, 50, true, 1:28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = optimise_upper!(ornn_optim, nn, Zmu, Zlogit_s, pars, trainIter2, opt, 5e-4, 50, true, 1:28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = optimise_upper!(ornn_optim, Zmap, trainIter2, opt, 7e-4, 100, true, 1:28)\n",
    "history3 = optimise_upper!(ornn_optim, Zmap, trainIter2, opt, 5e-4, 100, true, 1:28)\n",
    "history4 = optimise_upper!(ornn_optim, Zmap, trainIter2, opt, 2e-4, 100, true, 1:28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = optimise_upper!(ornn_optim, Zmap, trainIter2, opt, 5e-4, 50, true, 1:28)\n",
    "history4 = optimise_upper!(ornn_optim, Zmap, trainIter2, opt, 2e-4, 50, true, 1:28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global optimisation of latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsmp = 300\n",
    "_Zsmp = cholesky(cov(Zmu.data')).U * f32(AxUtil.Random.sobol_gaussian(nsmp, k)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate error matrix with above samples\n",
    "res = ones(Float32, length(trainIter), nsmp)\n",
    "\n",
    "ornn_optim_ng = model.make_nograd(ornn_optim);\n",
    "rnn_ng = mapleaves(Tracker.data, RNN(d_in, d_state, ornn_optim.σ))\n",
    "@time for i in 1:nsmp\n",
    "    _ψ = nn_ng(_Zsmp[:,i]);\n",
    "    c_ornn = model.make_rnn_psi(ornn_optim_ng, _ψ, 1f0)\n",
    "    model.build_rnn!(rnn_ng, c_ornn)\n",
    "    for (n, (Yb, Ub, h0)) in enumerate(trainIter2)\n",
    "        h0 && Flux.reset!(rnn_ng)\n",
    "        Tb = size(Yb, 2)\n",
    "        x̂ = reduce(hcat, [rnn_ng(Ub[:,i]) for i in 1:Tb])\n",
    "        ŷ = let m=c_ornn; m.C*x̂ + m.D*Ub .+ m.d; end\n",
    "        res[n,i] = mean(x->x^2, Yb - ŷ)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from implicit posterior (SNIS)\n",
    "pz = softmax(-32*(res')) \n",
    "z_smpopt = copy(Zmu.data)\n",
    "for i in 1:length(trainIter2)\n",
    "    z_smpopt[:,i] = _Zsmp[:, argmax(pz[:,i])]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to compare with current position\n",
    "ax = gca()\n",
    "# ax.scatter(_Zsmp[:,1], _Zsmp[:,2], alpha=0.1)\n",
    "for i in 1:7\n",
    "    ixs = segment_lkp[i]\n",
    "    z = z_smpopt[:, ixs] .+ randn(Float32, k, length(ixs))*0.005\n",
    "    ax.scatter(z[2,:], z[3,:], color=ColorMap(\"tab10\")(i-1), alpha=0.5)\n",
    "end\n",
    "legend(style_names[(1:7) .+ 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to compare with current position\n",
    "ax = gca()\n",
    "for i in 1:7\n",
    "    ixs = segment_lkp[i]\n",
    "    ax.scatter(Zmu.data[2, ixs], Zmu.data[3,ixs], color=ColorMap(\"tab10\")(i-1), alpha=0.5)\n",
    "end\n",
    "legend(style_names[(1:7) .+ 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update latents\n",
    "# error(\"safeguard\")\n",
    "Zmu.data .= z_smpopt .+ randn(Float32, k, length(trainIter))*0.005;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error(\"safeguard\")\n",
    "fname = format(\"ornn{:d}_2L_{:d}_{:d}_pool_{:02d}{:02d}_var.bson\", style_ix, d_state, \n",
    "    k, day(today()), month(today()))\n",
    "BSON.bson(fname, m2=ornn_optim_ng, nn=nn_ng, Zmu=Zmu.data, Zls=σ.(3*Zlogit_s.data));\n",
    "println(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ornn_base, ornn_optim_ng, nn_ng, Zmap = let b=BSON.load(\"ornn_2L_100_2_pool_1806_v64.bson\"); \n",
    "#     b[:m1], b[:m2], b[:nn], b[:Zmap]; end\n",
    "# ornn_base = model.make_grad(ornn_base)\n",
    "# ornn_optim = model.make_grad(ornn_optim_ng)\n",
    "# ornn_optim_ng = model.make_nograd(ornn_optim)\n",
    "# Zmap = Flux.param(Zmap);\n",
    "# nn = mapleaves(Tracker.param, nn_ng)\n",
    "# nn_ng = mapleaves(Tracker.data, nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_i = 50\n",
    "n_draws = 3\n",
    "\n",
    "_Yb = reduce(hcat, [trainIters[dset_i+i][1] for i in 0:2])  #[:,5:end]\n",
    "_U2 = reduce(hcat, [trainIters2[dset_i+i][2] for i in 0:2]) #[:,1:end-4]\n",
    "_Tb = size(_Yb, 2)\n",
    "# _eps = cholesky(cov(Zmap.data')).U * randn(Float32, 2, n_draws)\n",
    "_eps = Tracker.data(Zmap)[:, rand(1:length(trainIter), n_draws)]\n",
    "_eps[:,1] = Tracker.data(Zmap)[:,dset_i]\n",
    "\n",
    "cldsY = map(1:n_draws) do i\n",
    "    _ψ = nn_ng(_eps[:,i]);  \n",
    "    c_ornn = model.make_rnn_psi(ornn_optim_ng, _ψ, 1f0)\n",
    "    c_ornn(_U2)\n",
    "#     c_ornn = model.make_rnn_psi(ornn_optim_ng, _ψ, 1f0)\n",
    "#     model.build_rnn!(rnn_ng, c_ornn)\n",
    "#     x̂ = reduce(hcat, [rnn_ng(_Ub[:,i]) for i in 1:_Tb])\n",
    "#     let m=c_ornn; m.C*x̂ + m.D*_Ub .+ m.d; end\n",
    "end\n",
    "\n",
    "fig, axs = subplots(5,4,figsize=(10,10))\n",
    "offset = 20\n",
    "for i = 1:20\n",
    "    axs[:][i].plot(_Yb'[:, i+offset])\n",
    "    for j in 1:n_draws\n",
    "        axs[:][i].plot(cldsY[j]'[:, i+offset], alpha=0.4)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise fit (and MT variability) for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_i = 30\n",
    "n_draws = 3\n",
    "\n",
    "trainIters = collect(trainIter);\n",
    "_Yb, _Ub, _h = trainIters[dset_i]\n",
    "_Tb = size(_Yb, 2)\n",
    "_eps = cholesky(cov(Zmap.data')).U * randn(Float32, 2, n_draws)\n",
    "_eps = Zmap.data[:, rand(Categorical(ones(length(trainIter))/length(trainIter)), n_draws)]\n",
    "_eps[:,1] = Zmap.data[:,dset_i]\n",
    "cldsY = map(1:n_draws) do i\n",
    "    _ψ = nn_ng(_eps[:,i]);    \n",
    "    c_ornn = model.make_rnn_psi(ornn_optim_ng, _ψ, 1f0)\n",
    "    model.build_rnn!(rnn_ng, c_ornn)\n",
    "    x̂ = reduce(hcat, [rnn_ng(_Ub[:,i]) for i in 1:_Tb])\n",
    "    let m=c_ornn; m.C*x̂ + m.D*_Ub .+ m.d; end\n",
    "end\n",
    "\n",
    "fig, axs = subplots(5,4,figsize=(10,10))\n",
    "offset = 40\n",
    "for i = 1:20\n",
    "    axs[:][i].plot(_Yb'[:, i+offset])\n",
    "    for j in 1:n_draws\n",
    "        axs[:][i].plot(cldsY[j]'[:, i+offset], alpha=0.4)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "nB = length(trainIter)\n",
    "plot(sqrt.(DSP.conv(history1, Windows.rect(nB))[nB:end-nB+1]/nB))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
