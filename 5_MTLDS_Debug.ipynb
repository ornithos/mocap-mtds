{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Revise\n",
    "using PyPlot, AxPlot, AxUtil, InferGMM\n",
    "using Distributions, Random\n",
    "using Formatting, Dates\n",
    "using Flux\n",
    "using Flux: Tracker, params\n",
    "using Flux.Tracker: @grad\n",
    "\n",
    "using Parameters, ArgCheck\n",
    "using StatsFuns, ProgressMeter\n",
    "\n",
    "include(\"../mtds-julia/mtdsutil.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous MTDS script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "const N = 20\n",
    "const TS_LEN = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "unsqueeze(xs, dim) = reshape(xs, (size(xs)[1:dim-1]..., 1, size(xs)[dim:end]...));\n",
    "issquare(A) = let sz = size(A); length(sz) == 2 && sz[1] == sz[2]; end\n",
    "eye(d) = Float64.(Array(I ,d, d)) \n",
    "trans_matrix_2d(θ, ρ) = ρ .* [cos(θ) sin(θ); -sin(θ) cos(θ)];\n",
    "\n",
    "function evaluate_state_deterministic(A, x0, timesteps)\n",
    "    @assert issquare(A)\n",
    "    d = size(A, 1)\n",
    "    X = zeros(d, timesteps+1)\n",
    "    X[:,1] = x0\n",
    "    for tt = 1:timesteps\n",
    "        X[:,tt+1] = A * X[:,tt]\n",
    "    end\n",
    "    return X\n",
    "end;\n",
    "\n",
    "Random.seed!(80)\n",
    "const Ctrue = [1. 0.]\n",
    "p_ρ_log = Uniform(4, 80)\n",
    "p_θ = Uniform((6.75/360) * 2π, (36/360) * 2π)\n",
    "x0 = [0 1]\n",
    "\n",
    "d = 2   # reqd later?\n",
    "seq_deterministic = []\n",
    "seq_noise = []\n",
    "\n",
    "rsob = AxUtil.Random.uniform_rand_sobol(N, [p_θ.a, p_θ.b], [p_ρ_log.a, p_ρ_log.b])\n",
    "for nn = 1:N\n",
    "    _A = trans_matrix_2d(rsob[nn,1], exp(log(0.5)/rsob[nn,2]))\n",
    "    Xcur = evaluate_state_deterministic(_A, x0, TS_LEN-1)'*Ctrue'\n",
    "    push!(seq_deterministic, Xcur[:])\n",
    "    push!(seq_noise,  Xcur[:] .+ randn(TS_LEN)*0.05)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterise system matrix to have spectral radius <= 1\n",
    "function cayley_suborthog(x, d)\n",
    "    n_skew = Int(d*(d-1)/2)\n",
    "    x_U, x_Vt, x_S = x[1:n_skew], x[n_skew+1:2*n_skew], x[2*n_skew+1:end]\n",
    "    U = AxUtil.Math.cayley_orthog(x_U, d)\n",
    "    Vt = AxUtil.Math.cayley_orthog(x_Vt, d)\n",
    "    S = AxUtil.Flux.diag0(σ.(x_S))\n",
    "    return U * S * Vt\n",
    "end\n",
    "\n",
    "function cayley_suborthog_easy(x, d)\n",
    "    n_skew = Int(d*(d-1)/2)\n",
    "    x_U, x_S = x[1:n_skew], x[1*n_skew+1:end]\n",
    "    U = cayley_orthog(x_U, d)\n",
    "    Vt = eye(d)\n",
    "    S = diag0(σ.(x_S))\n",
    "    return U * S * Vt\n",
    "end\n",
    "\n",
    "feat_extract(z) = reduce(vcat, [z ,sin.(z), cos.(z), sqrt.(sum(x->x^2, z, dims=1))])\n",
    "feat_extract(z::TrackedArray) = Tracker.track(feat_extract, z)\n",
    "\n",
    "\n",
    "function feat_extract_deriv(Δ, f, d)\n",
    "    out = Δ[1:d,:]\n",
    "    for dd in 1:d\n",
    "        out[dd,:] .+= (f[d*2+dd,:] .* Δ[d+dd,:])   # cos z_dd * ∇_{d+dd}\n",
    "        out[dd,:] .+= -(f[d*1+dd,:] .* Δ[2*d+dd,:])  # - sin z_dd * ∇_{2d+dd}\n",
    "        out[dd,:] .+= (f[dd,:] .* Δ[3*d+1,:]) ./ (f[3*d+1,:] .+ 1e-16) # (z_dd /||z||) * ∇_{3d+1}\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "@grad function feat_extract(z::AbstractVector)\n",
    "    zd = Tracker.data(z)\n",
    "    d = size(z, 1)\n",
    "    f = feat_extract(zd)    \n",
    "    return f, Δ->let g=feat_extract_deriv(Δ, f, d); (vec(g), ); end\n",
    "end\n",
    "\n",
    "@grad function feat_extract(z::AbstractMatrix)\n",
    "    zd = Tracker.data(z)\n",
    "    d = size(z, 1)\n",
    "    f = feat_extract(zd)    \n",
    "    return f, Δ->let g=feat_extract_deriv(Δ, f, d); (g, ); end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only optimising nn and logσ for DHO1X problems. B is fixed.\n",
    "struct DhoModel{T <: AbstractFloat, F <: Chain}\n",
    "    nn::F\n",
    "    logσ::Tracker.TrackedVector{T}\n",
    "end\n",
    "\n",
    "struct DhoModelUntracked{T <: AbstractFloat, F <: Chain}\n",
    "    nn::F\n",
    "    logσ::Vector{T}\n",
    "end\n",
    "pars(x::DhoModel) = params([x.nn, x.logσ]...)\n",
    "make_untracked(x::DhoModel) = DhoModelUntracked(mapleaves(Tracker.data, x.nn), Tracker.data(x.logσ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_pars(x::DhoModel)\n",
    "    weights = Tracker.data.(params(x.nn))\n",
    "    weights = vcat(map(vec, weights)...)\n",
    "    return vcat(weights, x.logσ.data[:])\n",
    "end\n",
    "\n",
    "function set_pars!(x::DhoModel{T,F}, p::Vector{T}) where {T <: AbstractFloat, F <: Chain}\n",
    "    weights = Tracker.data.(params(x.nn))\n",
    "    sz = map(size, weights)\n",
    "    csz = cumsum([0; map(prod, sz)])\n",
    "    new_weights = [reshape(p[(csz[nn]+1):csz[nn+1]], sz[nn]...) for nn in 1:length(sz)]\n",
    "    Flux.loadparams!(x.nn, new_weights)\n",
    "    x.logσ.data .= p[end];\n",
    "end\n",
    "\n",
    "function get_grad(x::DhoModel)\n",
    "    weights = Tracker.grad.(params(x.nn))\n",
    "    weights = vcat(map(vec, weights)...)\n",
    "    return vcat(weights, x.logσ.data[:])\n",
    "end\n",
    "\n",
    "function zero_grad!(x::DhoModel)\n",
    "    ps = params(x.nn)\n",
    "    for p in ps\n",
    "        p.tracker.grad .= 0\n",
    "    end\n",
    "    x.logσ.grad .= 0\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Tracker: TrackedVector\n",
    "\n",
    "mutable struct lds_internal_notrk{T <: AbstractFloat}\n",
    "    x::Vector{T}\n",
    "    x_prev::Vector{T}\n",
    "end\n",
    "\n",
    "\n",
    "mutable struct lds_internal{T <: AbstractFloat}\n",
    "    x::TrackedVector{T}\n",
    "    x_prev::TrackedVector{T}\n",
    "end\n",
    "\n",
    "function A(ψ, d)\n",
    "    n_skew = Int(d*(d-1)/2)\n",
    "    x_S, x_V = ψ[1:d], ψ[d+1:2+n_skew]\n",
    "    V = AxUtil.Math.cayley_orthog(x_V/10, d)\n",
    "    S = AxUtil.Flux.diag0(tanh.(x_S))    # <= NOTICE SWITCHED FOR TANH\n",
    "    return S * V\n",
    "end\n",
    "\n",
    "function B(ψ, d)\n",
    "    n_skew = Int(d*(d-1)/2)\n",
    "    ψ_b = ψ[(d+n_skew+1):(d+n_skew+d)]\n",
    "    return identity.(ψ_b)\n",
    "end\n",
    "\n",
    "function C(ψ, d)\n",
    "    n_skew = Int(d*(d-1)/2)\n",
    "    ψ_b = ψ[(2*d+n_skew+1):(2*d+n_skew+d)]\n",
    "    return reshape(ψ_b, 1, d)\n",
    "end\n",
    "\n",
    "function _harm_osc2d_state(λ::Vector{T}; tt=TS_LEN)::Matrix{T} where T <: AbstractFloat\n",
    "    d = 2\n",
    "    h0 = B(λ, d) * T(1)\n",
    "    _A = A(λ, d)\n",
    "    hidden = lds_internal_notrk{T}(h0, h0)\n",
    "    function iterate(state)::Vector{T}\n",
    "        state.x_prev = state.x\n",
    "        state.x = _A*state.x; \n",
    "        state.x_prev\n",
    "    end\n",
    "    \n",
    "    lds_state = [iterate(hidden) for t in 1:tt]\n",
    "    return hcat(lds_state...)\n",
    "end\n",
    "\n",
    "function _harm_osc2d_state(λ::TrackedVector{T}; tt=TS_LEN)::TrackedMatrix{T} where T <: AbstractFloat\n",
    "    d = 2\n",
    "    h0 = B(λ, d) * T(1)\n",
    "    _A = A(λ, d)\n",
    "    lds_cell = AxUtil.Flux.LDSCell_simple(_A, h0)\n",
    "    lds = Flux.Recur(lds_cell, h0, h0)  # Flux constructor can't find hidden method def outside Flux\n",
    "    lds_state = [i == 1 ? lds.state : lds(1) for i in 1:tt]\n",
    "    return hcat(lds_state...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function _model_forward(model::Union{DhoModel, DhoModelUntracked}, Z::AbstractMatrix{T}; tt=TS_LEN) where T <: AbstractFloat\n",
    "#     d, M = size(Z, 1), size(Z, 2)   # explicit to ensure always gives 2nd dim (even if none := 1)\n",
    "#     Λ = model.nn(feat_extract(Z))\n",
    "#     Y = map(1:M) do m\n",
    "#         x = _harm_osc2d_state(Λ[:,m]; tt=tt)\n",
    "#         ŷ = C(Λ[:,m], d) * x\n",
    "#     end\n",
    "#     return reduce(vcat, Y)\n",
    "# end\n",
    "\n",
    "# NO FEATURE EXTRACTOR!\n",
    "function _model_forward(model::Union{DhoModel, DhoModelUntracked}, Z::AbstractMatrix{T}; tt=TS_LEN) where T <: AbstractFloat\n",
    "    d, M = size(Z, 1), size(Z, 2)   # explicit to ensure always gives 2nd dim (even if none := 1)\n",
    "    Λ = model.nn(Z)\n",
    "    Y = map(1:M) do m\n",
    "        x = _harm_osc2d_state(Λ[:,m]; tt=tt)\n",
    "        ŷ = C(Λ[:,m], d) * x\n",
    "    end\n",
    "    return reduce(vcat, Y)\n",
    "end\n",
    "\n",
    "# Union type dispatch is unavailable: use separate methods pointing to same function\n",
    "(model::DhoModel)(Z::AbstractMatrix; tt::Int=TS_LEN) = Tracker.collect(_model_forward(model, Z; tt=tt))\n",
    "(model::DhoModelUntracked)(Z::Matrix; tt::Int=TS_LEN) = _model_forward(model, Z; tt=tt)\n",
    "(model::DhoModelUntracked)(Z::TrackedMatrix; tt::Int=TS_LEN) = Tracker.collect(_model_forward(model, Z; tt=tt))\n",
    "\n",
    "# Vector input --> Matrix\n",
    "(model::DhoModel)(Z::AbstractVector; tt::Int=TS_LEN) = Tracker.collect(_model_forward(model, unsqueeze(Z,2); tt=tt))\n",
    "(model::DhoModelUntracked)(Z::Vector; tt::Int=TS_LEN) = _model_forward(model, unsqueeze(Z,2); tt=tt)\n",
    "(model::DhoModelUntracked)(Z::TrackedVector; tt::Int=TS_LEN) = Tracker.collect(_model_forward(model, unsqueeze(Z,2); tt=tt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sq_diff_matrix(X, Y)\n",
    "    #=\n",
    "    Constructs $(x_i - y_j)^T (x_i - y_j)$ matrix where\n",
    "    X = Array(n_x, d)\n",
    "    Y = Array(n_y, d)\n",
    "    return: Array(n_x, n_y)\n",
    "    =#\n",
    "    normsq_x = sum(a -> a^2, X, dims=2)\n",
    "    normsq_y = sum(a -> a^2, Y, dims=2)\n",
    "    @assert size(normsq_x, 2) == 1 && size(normsq_y, 2) == 1\n",
    "    out = normsq_x .+ normsq_y'    # outer\n",
    "    @assert size(out) == (size(normsq_x, 1), size(normsq_y, 1))\n",
    "    out .-= 2*X * Y'\n",
    "    return out\n",
    "end\n",
    "\n",
    "function llh_weight_matrix(X, Xmu, Dlogdiag; incl_const=True)\n",
    "    #=\n",
    "    constructs log likelihood matrix of all combinations\n",
    "    of all rows of X and Xmu (or 'Xhat') with diagonal cov\n",
    "    for which the log diagonal vector `Dlogdiag' is specified.\n",
    "    =#\n",
    "    @assert ndims(Dlogdiag) == 1\n",
    "    Dsqrtdiag = exp.(Dlogdiag*0.5)\n",
    "    d = size(Dsqrtdiag, 1)\n",
    "    X = X ./ Dsqrtdiag'      # consider in-place operation? e.g. https://github.com/simonbyrne/InplaceOps.jl\n",
    "    Xmu = Xmu ./ Dsqrtdiag'  # consider in-place operation?\n",
    "    out = -0.5 * sq_diff_matrix(X, Xmu)\n",
    "    if incl_const\n",
    "        out .+= -0.5*d*log(2π) - 0.5*sum(Dlogdiag)\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "function snis_weight_matrix(X, Xhat, Dlogdiag)\n",
    "    #=\n",
    "    Self Normalised Importance Weights for full cross comparison btwn `X`\n",
    "    and `Xhat`. The matrix is returned as numrows(X) x numrows(Xhat).\n",
    "    \n",
    "    While we're at it, it makes sense to calculate an estimate of the log\n",
    "    probability, so the first return value is the SNIS matrix, the second\n",
    "    is the approx log probability of each row.\n",
    "    =#\n",
    "    W = Matrix(llh_weight_matrix(X, Xhat, Dlogdiag, incl_const=true)')\n",
    "    W, lse = AxUtil.Math.softmax_lse!(W)   # softmax the rows, calc logp\n",
    "    W = W'  # softmax does cols, we want rows!\n",
    "    return W, lse .- log(size(Xhat, 1))  # W, log(1/M sum_j exp(log_j))\n",
    "end\n",
    "\n",
    "function get_posterior_smp_matrix(model::DhoModelUntracked, n::Int, seq_obs; tt=TS_LEN)\n",
    "    Zproposal = AxUtil.Random.sobol_gaussian(n, 2)'\n",
    "    Xhat = model(Zproposal; tt=tt);\n",
    "    Xtrue = reduce(vcat, map(x->x', seq_obs));\n",
    "    w, lp = snis_weight_matrix(Xtrue, Xhat, ones(tt)*model.logσ[1]*2);\n",
    "    return Zproposal, w, lp\n",
    "end\n",
    "\n",
    "get_log_px(model::DhoModelUntracked, n::Int, seq_obs; tt=TS_LEN) = get_posterior_smp_matrix(\n",
    "    model, n, seq_obs; tt=tt)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- LLH for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "const prior_var = 1.0\n",
    "\n",
    "function p_log_llh(model::Union{DhoModel, DhoModelUntracked}, Y::Vector, Z::AbstractVecOrMat; tt=TS_LEN)\n",
    "    Ŷ = model(Z, tt=tt)\n",
    "    @argcheck size(Y, 1) == size(Ŷ,2)\n",
    "    Δ = (Ŷ .- Y') ./ exp(model.logσ[1])\n",
    "    return -0.5*sum(Δ.^2, dims=2)[:]\n",
    "end\n",
    "\n",
    "function p_log_prior(Z)\n",
    "    d = size(Z,1)\n",
    "    @argcheck d == 2\n",
    "    ZZ = Z .* 1/sqrt(prior_var)\n",
    "    exponent = -0.5*sum(ZZ.^2, dims=1)\n",
    "    return exponent[:]\n",
    "end\n",
    "\n",
    "function p_log_posterior_unnorm(model::DhoModelUntracked, Y::Vector, Z::AbstractVecOrMat; tt=TS_LEN)\n",
    "    f_llh = p_log_llh(model, Y, Z; tt=tt)\n",
    "    f_prior = p_log_prior(Z)\n",
    "    return f_llh + f_prior\n",
    "end\n",
    "\n",
    "function p_log_posterior_unnorm_beta(model::DhoModelUntracked, Y::Vector, Z::AbstractVecOrMat, beta::AbstractFloat)\n",
    "    f_llh = p_log_llh(model, Y, Z)\n",
    "    f_prior = p_log_prior(Z)\n",
    "    return f_prior + beta*f_llh, f_llh + f_prior\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "d_out = (d+1) + d + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = (d+1) + d + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_deterministic = []\n",
    "seq_noise = []\n",
    "\n",
    "# Random.seed!(5921207)\n",
    "Random.seed!(2220175)  # explored\n",
    "# Random.seed!(3197275)  # this one has the low θ example (and not an esp low one!). Clearly highly curved. Suggest look at PCs.\n",
    "# Random.seed!(20175)\n",
    "N = 4\n",
    "rsob = AxUtil.Random.uniform_rand_sobol(N, [p_θ.a, p_θ.b], [p_ρ_log.a, p_ρ_log.b])\n",
    "\n",
    "for nn = 1:N\n",
    "    _A = trans_matrix_2d(rsob[nn,1], exp(log(0.5)/rsob[nn,2]))\n",
    "    Xcur = evaluate_state_deterministic(_A, x0, TS_LEN-1)'*Ctrue'\n",
    "    push!(seq_deterministic, Xcur[:])\n",
    "    push!(seq_noise,  Xcur[:] .+ randn(TS_LEN)*0.05)\n",
    "end\n",
    "\n",
    "d_nn = 300   # hidden layer of neural net link fn\n",
    "d_nn_in = 2 # 7\n",
    "# linkfn = Chain(Dense(d_nn_in, d_nn, σ), Dense(d_nn, d_out, identity));\n",
    "linkfn = Chain(Dense(d_nn_in, d_nn, σ), Dense(d_nn, d_out, identity), Flux.Diagonal(d_out));\n",
    "log_emission_std = [-1.5]\n",
    "\n",
    "dho_model = DhoModel(linkfn, Flux.param(Float32.(log_emission_std)))\n",
    "dho_model_nog = make_untracked(dho_model)\n",
    "\n",
    "nsmps = 500\n",
    "epochs = 500\n",
    "n_est = 5\n",
    "cpars = pars(dho_model)\n",
    "prior_lsigma = Normal(-1.3, 0.04)\n",
    "\n",
    "opt = ADAM(8e-4, (0.9,0.99));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history6, opt, pps = evidence_optimisation_async(dho_model, dho_model_nog, seq_noise, cpars; epochs=100, \n",
    "                        n_est=5, nsmps=1000, prior_lsigma=prior_lsigma, norm_clip_amt=1000., opt=opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = subplots(1,2,figsize=(9,4))\n",
    "for i in 1:2\n",
    "    axs[i].plot(history[:,1])\n",
    "    axs[i].plot(history2[:,1])\n",
    "    (i == 1) ? axs[i].plot(history3[:,1]) : axs[i].plot(0,0)\n",
    "    axs[i].plot(history4[:,1])\n",
    "    axs[i].plot(history5[:,1])\n",
    "    axs[i].plot(history6[:,1])\n",
    "end\n",
    "axs[1].legend([L\"$\\eta(\\cdot), C(\\sigma)$\", L\"$(\\cdot), C(\\sigma)$\", \n",
    "            L\"$(\\cdot), C(\\sigma)$, async\", L\"$(\\cdot), C(\\cdot)$\", L\"$(\\cdot), C(\\cdot), elu$\",\n",
    "            L\"$(\\cdot), C(\\cdot)$, no diaglayer\"])\n",
    "axs[2].set_ylim([20,34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history4, opt, pps = evidence_optimisation_async(dho_model, dho_model_nog, seq_noise, cpars; epochs=20, \n",
    "#                         n_est=5, nsmps=1000, prior_lsigma=prior_lsigma, norm_clip_amt=1000., opt=opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "function evidence_optimisation_async(dho_model::DhoModel, dho_model_nog::DhoModelUntracked, seq_obs, pars; \n",
    "                               epochs=100, nsmps=1000, n_est=1, prior_lsigma=Normal(0, 100), norm_clip_amt=1000.,\n",
    "                                opt=ADAM(1e-3), acc_step=false)\n",
    "    # return opt, history, ...\n",
    "    verbose = false\n",
    "    N = length(seq_obs)\n",
    "    history = zeros(epochs, 25)\n",
    "    allpar = Any[]\n",
    "    \n",
    "    AxUtil.Flux.zero_grad!(pars)   # just in case have previously been held and have hidden gradient.\n",
    "    cparval = [get_pars(dho_model), get_pars(dho_model)]\n",
    "    \n",
    "    g_ν = zeros(300)\n",
    "    for i in 1:epochs\n",
    "        \n",
    "        # --- Backprop through posterior-integrated likelihood ---\n",
    "        recon_loss = 0.\n",
    "        \n",
    "        for nn in 1:N\n",
    "            # --- Generate approximate posterior samples ---\n",
    "            Zproposal, w, logp_s = get_posterior_smp_matrix(dho_model_nog, nsmps, seq_obs[nn:nn]; tt=TS_LEN)\n",
    "            logp = sum(logp_s)\n",
    "\n",
    "            smps = AxUtil.Random.multinomial_indices_linear(n_est, w[:])\n",
    "            smps = reduce(vcat, smps)\n",
    "            verbose && println(smps')\n",
    "            zsmps_nn = Zproposal[:, smps]\n",
    "            # --- /end ---\n",
    "\n",
    "            # neg. log likelihood / reconstruction\n",
    "            loss = -sum(p_log_llh(dho_model, seq_obs[nn], zsmps_nn; tt=TS_LEN))\n",
    "            loss /= n_est   # normalise for number of samples\n",
    "            recon_loss += loss.data\n",
    "            verbose && println(loss.data)\n",
    "            \n",
    "            # Likelihood normalising constant  (# not n*T*logσ as we have normalised by n already)\n",
    "            loss += sum(TS_LEN*dho_model.logσ)\n",
    "\n",
    "            # Log Normal prior on log sigma\n",
    "            loss += 0.5*sum(x->x^2, dho_model.logσ .- prior_lsigma.μ)/prior_lsigma.σ^2\n",
    "            verbose && println(loss.data)\n",
    "            Tracker.back!(loss)\n",
    "            \n",
    "            # Regularise MLP weights\n",
    "            loss += 1e-3*sum(abs, dho_model.nn.layers[1].W) / N\n",
    "            loss += 1e-1*sum(abs, dho_model.nn.layers[2].W) / N\n",
    "\n",
    "            for p in pars\n",
    "                Tracker.update!(opt, p, -Tracker.grad(p))\n",
    "            end\n",
    "        end\n",
    "        # --- /end ---\n",
    "        \n",
    "        __zp, __w, logp_s = get_posterior_smp_matrix(dho_model_nog, nsmps, seq_obs; tt=TS_LEN)\n",
    "        logp = sum(logp_s)\n",
    "        \n",
    "        logrecon = -recon_loss - N * (TS_LEN*(dho_model.logσ.data[1] + log(2π)/2))\n",
    "        \n",
    "        history[i,1:2] = [logp, logrecon]/N\n",
    "        if (i % 5 == 1) \n",
    "            print(\"Batch iter \", i, \", recon loss: \", sprintf1(\"%.3f\", logrecon/N))\n",
    "            println(\", prev logp(x): \", sprintf1(\"%.2f\", logp/N))\n",
    "#             flush(stdout);\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return history, opt, allpar\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "function evidence_optimisation_sync(dho_model::DhoModel, dho_model_nog::DhoModelUntracked, seq_obs, pars; \n",
    "                               epochs=100, nsmps=1000, n_est=1, prior_lsigma=Normal(0, 100), norm_clip_amt=1000.,\n",
    "                                opt=ADAM(1e-3), acc_step=false)\n",
    "    # return opt, history, ...\n",
    "    N = length(seq_obs)\n",
    "    history = zeros(epochs, 25)\n",
    "    allpar = Any[]\n",
    "    \n",
    "    AxUtil.Flux.zero_grad!(pars)   # just in case have previously been held and have hidden gradient.\n",
    "    cparval = [get_pars(dho_model), get_pars(dho_model)]\n",
    "    \n",
    "    g_ν = zeros(300)\n",
    "    nclips = 0\n",
    "    for i in 1:epochs\n",
    "\n",
    "        # --- Generate approximate posterior samples ---\n",
    "        Zproposal, w, logp_s = get_posterior_smp_matrix(dho_model_nog, nsmps, seq_obs; tt=TS_LEN)\n",
    "        logp = sum(logp_s)\n",
    "\n",
    "        smps = [AxUtil.Random.multinomial_indices_linear(n_est, view(w, i, :)) for i ∈ 1:N]\n",
    "        smps = reduce(vcat, smps)\n",
    "        Zs_post = Zproposal[:, smps]\n",
    "        # --- /end ---\n",
    "\n",
    "        # --- Backprop through posterior-integrated likelihood ---\n",
    "        epoch_loss = 0.\n",
    "        recon_loss = 0.\n",
    "        for nn in 1:N\n",
    "            zsmps_nn = Zs_post[:, (nn-1)*n_est+1:nn*n_est]\n",
    "\n",
    "            # neg. log likelihood / reconstruction\n",
    "            loss = -sum(p_log_llh(dho_model, seq_obs[nn], zsmps_nn; tt=TS_LEN))\n",
    "            loss /= n_est   # normalise for number of samples\n",
    "            recon_loss += loss.data\n",
    "            \n",
    "            # Likelihood normalising constant  (# not n*T*logσ as we have normalised by n already)\n",
    "            loss += sum(TS_LEN*dho_model.logσ)\n",
    "\n",
    "            # Log Normal prior on log sigma\n",
    "            loss += 0.5*sum(x->x^2, dho_model.logσ .- prior_lsigma.μ)/prior_lsigma.σ^2\n",
    "\n",
    "            epoch_loss += loss.data\n",
    "            Tracker.back!(loss)\n",
    "        end\n",
    "        # --- /end ---\n",
    "\n",
    "#         nclips += AxUtil.Flux.normclip!(cpars, norm_clip_amt)\n",
    "        nclips = 0\n",
    "\n",
    "        # Regularise MLP weights\n",
    "        loss += 1e-3*sum(abs, dho_model.nn.layers[1].W)\n",
    "        loss += 1e-1*sum(abs, dho_model.nn.layers[2].W)\n",
    "        \n",
    "        norm_g = [norm(dho_model.nn.layers[2].W.grad[i,:]) for i in 1:7]\n",
    "        ν_angle = (dho_model.nn.layers[2].W.grad[3,:]'*g_ν)/(norm(g_ν)*norm_g[3])\n",
    "        g_ν = dho_model.nn.layers[2].W.grad[3,:]\n",
    "        for p in pars\n",
    "            Tracker.update!(opt, p, -Tracker.grad(p))\n",
    "        end\n",
    "#         newparval = get_pars(dho_model)\n",
    "#         push!(allpar, newparval)\n",
    "#         cos_optim = let g1=(cparval[2] - cparval[1]); g2=newparval-cparval[2]; sum(x->x^2, g1 .* g2)/(norm(g1)*norm(g2)); end\n",
    "#         cparval = [cparval[2], newparval]\n",
    "        \n",
    "        logrecon = -recon_loss - N * (TS_LEN*(dho_model.logσ.data[1] + log(2π)/2))\n",
    "        \n",
    "#         mqnt(x) = [mean(x), quantile(x, 0.1), quantile(x, 0.9)]\n",
    "#         _smps = dho_model_nog.nn(feat_extract(randn(2,1000)))\n",
    "#         _ρs = vcat([mqnt(σ.(_smps[i,:])) for i in 1:2]...)\n",
    "#         _θ = vcat([mqnt([2 .* atan.(t/10) .* 360 ./ 2π for t in _smps[i,:]]) for i in 3:3]...)\n",
    "#         _B_s = vcat([mqnt(σ.(_smps[i,:])) for i in 4:5]...)\n",
    "#         _C_s = vcat([mqnt(_smps[i,:]) for i in 4:5]...)\n",
    "        \n",
    "#         history[i, :] = vcat([logp, logrecon]/N,  _ρs, _θ, _B_s, _C_s, cos_optim, dho_model_nog.logσ[1])\n",
    "        history[i,1] = logp/N\n",
    "        history[i,1:25] = vcat([logp, logrecon]/N, norm_g, [ν_angle], g_ν[1:15])\n",
    "        if (i % 5 == 1) \n",
    "            print(\"Batch iter \", i, \", recon loss: \", sprintf1(\"%.3f\", logrecon/N))\n",
    "            println(\", prev logp(x): \", sprintf1(\"%.2f\", logp/N), \" (clip: \", \"|\"^nclips, \")\")\n",
    "            nclips = 0\n",
    "            flush(stdout)\n",
    "#             println(angle_stuff(dho_model_nog.nn)[:])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return history, opt, allpar\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we do the same thing using the Mocap machinery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_MOCAP_MTDS = \".\" \n",
    "\n",
    "# Data loading and transformation utils\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"io.jl\"))\n",
    "\n",
    "# MeshCat skeleton visualisation tools\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"mocap_viz.jl\"))\n",
    "\n",
    "# Data scaling utils\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"util.jl\"))\n",
    "import .mocaputil: MyStandardScaler, scale_transform, invert\n",
    "import .mocaputil: OutputDifferencer, difference_transform, fit_transform\n",
    "import .mocaputil: no_pos, no_poscp\n",
    "\n",
    "# Models: LDS\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"models.jl\"))\n",
    "import .model: Astable\n",
    "\n",
    "# Table visualisation\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"pretty.jl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE B, b, AND D\n",
    "drop_amt = 0.0\n",
    "\n",
    "function model._make_lds_psi(s::Union{model.MTLDS_g{T,F}, model.MTLDS_ng{T,F}},\n",
    "        ψ::Union{AbstractVector{T}, TrackedVector{T}},\n",
    "        η_h::Union{T, Vector{T}}=s.η_h) where {T <: Real, F <: Chain}\n",
    "    d_state, d_out, d_in = size(s)\n",
    "    ldsdims = model._partition_ldspars_dims(d_state, d_out, d_in, length(ψ))\n",
    "    a, B, b, C, D, d = model.partition_ldspars(ψ, ldsdims, d_state, d_out, d_in)\n",
    "    state = deepcopy(s.h)\n",
    "    ldstype = model.has_grad(s) ? model.MyLDS_g{T} : model.MyLDS_ng{T}\n",
    "    η₁ = model.arr2sc(s.η_h)\n",
    "    m_B = 1 #rand() > drop_amt ? 1 : 0   # B dropout\n",
    "    m_D = 0 #rand() > drop_amt ? 1 : 0   # D dropout\n",
    "    return ldstype(η₁*a + s.a, η₁*B*m_B*T(0.1) + s.B*m_B, η₁*s.b*m_B, C + s.C, D*0, d*m_D, state)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "##    CUSTOM WIDELY USED FUNCTIONS\n",
    "function zero_grad!(P) \n",
    "    for x in P\n",
    "        x.grad .= 0\n",
    "    end\n",
    "end\n",
    "\n",
    "rmse(Δ::AbstractArray, scale=size(Δ, 1)) = sqrt(mean(x->x^2, Δ))*scale\n",
    "\n",
    "function rmse(d::mocaputil.DataIterator, m::model.MyLDS_ng)\n",
    "    obj = map(d) do (y, u, new_state)\n",
    "        new_state && (m.h .= zeros(size(m, 1))) \n",
    "        rmse(m(u) - y)\n",
    "    end\n",
    "    m.h .= zeros(size(m, 1))\n",
    "    return dot(obj, mocaputil.weights(d, as_pct=true))\n",
    "end\n",
    "\n",
    "\n",
    "rmse(Ds::Vector{D}, m::model.MyLDS_ng) where {D <: Dict} = rmse(mocaputil.DataIterator(Ds, 1000000), m)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise LDS\n",
    "clds_orig = model.MyLDS_ng{Float32}(f32([zeros(d); zeros(Int(d*(d-1)/2))]), f32(zeros(d, 1)), f32(zeros(d)),\n",
    "            f32(zeros(1, d)), f32(zeros(1, 1)), f32(zeros(1)), f32([0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2                 # dimension of manifold\n",
    "d_nn = 300            # \"complexity\" of manifold\n",
    "\n",
    "d_par = length(model.get_pars(clds_orig))\n",
    "nn = Chain(Dense(k, d_nn, σ), \n",
    "           Dense(d_nn, d_par, identity, initW = ((dims...)->Flux.glorot_uniform(dims...)*1.0)),\n",
    "           Flux.Diagonal(d_par))\n",
    "clogσ = repeat([-1.5f0], size(clds_orig, 2))\n",
    "\n",
    "cmtlds_g = model.mtldsg_from_lds(clds_orig, nn, clogσ, 1.0f0);\n",
    "cmtlds = model.make_nograd(cmtlds_g);\n",
    "# model.change_relative_lr!(cmtlds, 0.001f0)   # reduce sensitivity of chain params. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "clds = model.make_lds(cmtlds, randn(Float32, 2), 0.1f0);\n",
    "seq_noise32 = [reshape(Float32.(y), 1, :) for y in seq_noise];\n",
    "Us32 = [hcat(1f0, zeros(Float32, 1, TS_LEN-1)) for i in 1:length(seq_noise32)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.change_relative_lr!(cmtlds, 0.10f0)   # reduce sensitivity of chain params. \n",
    "cmtlds_g.η_h .= 0.10f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random.seed!(15201)\n",
    "# opt = ADAM(8e-4, (0.9,0.99));\n",
    "nsmps = 300\n",
    "epochs = 30\n",
    "n_est = 3\n",
    "prior_lsigma = Normal(-1.3, 0.04)\n",
    "\n",
    "# opt = ADAM(4e-4, (0.9,0.99));\n",
    "# opt.eta /=2\n",
    "# opt.eta = 1e-10\n",
    "opt.eta = 8e-4\n",
    "history6, opt, pps = evidence_optimisation_new(cmtlds_g, seq_noise32, Us32[1]; epochs=epochs, \n",
    "                        n_est=n_est, nsmps=nsmps, prior_lsigma=prior_lsigma, opt=opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_grad!(cpars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random.seed!(15201)\n",
    "# opt.eta = 2e-3\n",
    "# cpars = pars(dho_model)\n",
    "opt.eta = 1e-4\n",
    "epochs=100\n",
    "nsmps = 400\n",
    "history6, opt, pps = evidence_optimisation_async(dho_model, dho_model_nog, seq_noise, cpars; epochs=epochs, \n",
    "                        n_est=n_est, nsmps=nsmps, prior_lsigma=prior_lsigma, norm_clip_amt=1000., opt=opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(15201)\n",
    "get_posterior_smp_matrix(dho_model_nog, nsmps, seq_noise[1:1]; tt=TS_LEN)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(15201)\n",
    "sample_posterior(cmtlds, seq_noise32[1], Us32[1], nsmps)[1]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_posterior_smp_matrix(dho_model_nog, nsmps, seq_noise[1:1]; tt=TS_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_posterior(cmtlds, seq_noise32[1], Us32[1], 3, f32(_zs[:, [193, 180, 141]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_log_llh(cmtlds_g, seq_noise32[1], Us32[1], f32(_zs[:, [193, 180, 141]]))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "AxUtil.Random.multinomial_indices_linear(3, vec(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand(Categorical(vec(w2)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "function evidence_optimisation_new(cmtlds_g::model.MTLDS_g, seq_obs, U, pars=model.pars(cmtlds_g); \n",
    "                               epochs=100, nsmps=1000, n_est=1, prior_lsigma=Normal(0, 100), norm_clip_amt=1000.,\n",
    "                                opt=ADAM(1e-3), acc_step=false, dropamt=0)\n",
    "    \n",
    "    verbose = false\n",
    "    cmtlds = model.make_nograd(cmtlds_g)\n",
    "    T = eltype(cmtlds)\n",
    "    N = length(seq_obs)\n",
    "    history = zeros(epochs, 2)\n",
    "    allpar = Any[]\n",
    "    \n",
    "    model.zero_grad!(cmtlds_g)\n",
    "#     cparval = [get_pars(dho_model), get_pars(dho_model)]\n",
    "    \n",
    "    g_ν = zeros(300)\n",
    "    nclips = 0\n",
    "    \n",
    "    for i in 1:epochs\n",
    "    # --- Backprop through posterior-integrated likelihood ---\n",
    "        recon_loss = zero(T)\n",
    "        logp = zero(T)\n",
    "        \n",
    "        for nn in 1:N\n",
    "            # --- Generate approximate posterior samples ---\n",
    "            w, logp_s, Zproposal = sample_posterior(cmtlds, seq_obs[nn], U, nsmps)\n",
    "            logp += sum(logp_s)\n",
    "            \n",
    "#             smps = AxUtil.Random.multinomial_indices_linear(n_est, w[:])\n",
    "#             smps = reduce(vcat, smps)\n",
    "#             verbose && println(smps')\n",
    "#             zsmps_nn = Zproposal[:, smps]\n",
    "#             Zs_wgt = ones(T, n_est)\n",
    "            smps = AxUtil.Random.multinomial_indices_linear(n_est, w[:])\n",
    "            # often duplicates (e.g. m_bprop of same sample), esp nr beginning. Aggregate to improve efficiency.\n",
    "            smps, smp_wgt = countmap(smps) |> x-> (collect(keys(x)), collect(values(x)))\n",
    "            zsmps_nn = Zproposal[:, smps]\n",
    "#             verbose && println(zsmps_nn')\n",
    "            Zs_wgt  = T.(smp_wgt)\n",
    "            # --- /end ---\n",
    "\n",
    "            # neg. log likelihood / reconstruction\n",
    "            llh, _state = p_log_llh(cmtlds_g, seq_obs[nn], U, zsmps_nn, Zs_wgt)\n",
    "            verbose && display(llh)\n",
    "            loss   = - sum(llh) /n_est  # *decrease* *negative* llh.\n",
    "#             println(loss.data)\n",
    "            recon_loss += -loss.data - TS_LEN*(sum(cmtlds.logσ) + size(cmtlds,2)*log(2π)/2)\n",
    "\n",
    "            # Likelihood normalising constant  (# not n*T*logσ as we have normalised by n already)\n",
    "            loss += TS_LEN*sum(cmtlds.logσ)\n",
    "            verbose && println(loss.data)\n",
    "            \n",
    "            # Log Normal prior on log sigma\n",
    "            loss += 0.5*sum(x->x^2, cmtlds.logσ .- prior_lsigma.μ) ./ prior_lsigma.σ^2\n",
    "            verbose && println(loss.data)\n",
    "            \n",
    "            Tracker.back!(loss)\n",
    "            \n",
    "            # Regularise MLP weights\n",
    "            loss += 1e-3*sum(abs, cmtlds_g.nn.layers[1].W) / N   # careful of doing layer 3 (lr rescale)\n",
    "#             loss += 1e-1*sum(abs, cmtlds_g.nn.layers[2].W) / N\n",
    "            \n",
    "#             display(Tracker.grad(cmtlds_g.nn.layers[end].α))\n",
    "            for p in pars\n",
    "                Tracker.update!(opt, p, -Tracker.grad(p))\n",
    "            end\n",
    "        end\n",
    "        # --- /end ---\n",
    "\n",
    "        history[i,1:2] = [logp, recon_loss]/N\n",
    "        if (i % 1 == 0) \n",
    "            print(\"Batch iter \", i, \", recon loss: \", sprintf1(\"%.3f\", recon_loss/N))\n",
    "            println(\", prev logp(x): \", sprintf1(\"%.2f\", logp/N))\n",
    "            flush(stdout)\n",
    "        #             println(angle_stuff(dho_model_nog.nn)[:])\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    # --- /end ---\n",
    "    \n",
    "    return history, opt, allpar\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample_forward(mtlds::model.MTLDS_ng{T,F}, U::AbstractArray{T}, M::Int, ϵ::AbstractMatrix{T}) where {T, F}\n",
    "    return [model.make_lds(mtlds, view(ϵ, :, i), mtlds.η_h)(U) for i in 1:M]\n",
    "end\n",
    "\n",
    "function sample_forward(mtlds::model.MTLDS_ng{T,F}, U::AbstractArray{T}, M::Int) where {T, F}\n",
    "    k = size(mtlds.nn.layers[1].W, 2)\n",
    "    ϵ = convert(Array{T}, AxUtil.Random.sobol_gaussian(M, k)')\n",
    "    return (sample_forward(mtlds, U, M, ϵ)..., ϵ)\n",
    "end\n",
    "\n",
    "\n",
    "@inline _gauss_lognormconst(logσ::Vector, tt) = 0.5*tt*length(logσ)*log(2π) + 0.5*tt*sum(2*logσ)\n",
    "\n",
    "#= slightly faster version for when a single Y (can do subtraction in-place).\n",
    "   This is called much more frequently during training (i.e. before every couple of batches) =#\n",
    "function sample_posterior(mtlds::model.MTLDS_ng{T,F}, Y::AbstractArray{T}, \n",
    "            U::AbstractArray{T}, M::Int, ϵ::AbstractArray{T}) where {T, F}\n",
    "    \n",
    "    Ŷ = sample_forward(mtlds, U, M, ϵ)\n",
    "    \n",
    "    # Calculate density of each sample Ŷ\n",
    "    Δnorm = Vector{T}(undef, M)\n",
    "    precision = 1 ./ exp.(2*mtlds.logσ)\n",
    "    for i in 1:M\n",
    "        @views Ŷ[i] .-= Y\n",
    "        Δnorm[i] = dot(sum(x->x^2, Ŷ[i], dims=2), precision)\n",
    "    end\n",
    "\n",
    "    # Elementwise logpdf ==> single logpdf and importance weights\n",
    "    el_lpdf = -0.5 * Δnorm .- _gauss_lognormconst(mtlds.logσ, size(Y,2))\n",
    "    W, lgpdf = AxUtil.Math.softmax_lse!(reshape(el_lpdf, :, 1))   # softmax AND logsumexp\n",
    "    return W, lgpdf[1] .- log(M)  # ϵ, W, log(1/M sum_j exp(log_j))\n",
    "end\n",
    "\n",
    "function sample_posterior(mtlds::model.MTLDS_ng{T,F}, Y::AbstractArray{T}, \n",
    "            U::AbstractArray{T}, M::Int) where {T, F}\n",
    "    k = size(mtlds.nn.layers[1].W, 2)\n",
    "    ϵ = convert(Array{T}, AxUtil.Random.sobol_gaussian(M, k)')\n",
    "    return (sample_posterior(mtlds, Y, U, M, ϵ)..., ϵ)\n",
    "end\n",
    "\n",
    "\n",
    "# slower version for *series of matrices* for Y (can no longer do in-place) \n",
    "function sample_posterior(mtlds::model.MTLDS_ng{T,F}, Ys::AbstractArray{MT}, \n",
    "            Us::AbstractArray{MT}, M::Int) where {T, F, MT <: Matrix}\n",
    "    \n",
    "    k = size(mtlds.nn.layers[1].W, 2)\n",
    "    \n",
    "    # common r.v.s\n",
    "    ϵ = convert(Array{T}, AxUtil.Random.sobol_gaussian(M, k)')\n",
    "    W, lgpdf = sample_posterior(mtlds, Ys, Us, M, ϵ)\n",
    "    \n",
    "    return Matrix(W'), lgpdf, ϵ  # W, log(1/M sum_j exp(log_j)), ϵ\n",
    "end\n",
    "\n",
    "function sample_posterior(mtlds::model.MTLDS_ng{T,F}, Ys::AbstractArray{MT}, \n",
    "            Us::AbstractArray{MT}, M::Int, ϵ::AbstractArray{T}) where {T, F, MT <: Matrix}\n",
    "    \n",
    "    N = length(Ys)\n",
    "    W = Matrix{T}(undef, N, M)\n",
    "    lgpdf = Vector{T}(undef, N)\n",
    "    \n",
    "    for nn in 1:N\n",
    "        _W, _lp = sample_posterior(mtlds, Ys[nn], Us[nn], M, ϵ)\n",
    "        W[nn,:] = _W[:]\n",
    "        lgpdf[nn] = _lp\n",
    "    end\n",
    "    \n",
    "    return Matrix(W'), lgpdf  # W, log(1/M sum_j exp(log_j)), ϵ\n",
    "end\n",
    "\n",
    "function get_log_px(model::model.MTLDS_ng{T,F}, Y::AbstractArray, U::AbstractArray, m::Int) where {T,F}\n",
    "    sample_posterior(model, Y, U, m)[2]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "const prior_var = 1.0\n",
    "\"\"\"\n",
    "    p_log_llh(mtlds, Y, U, Z, wgt=ones(T, size(Z, 2)))\n",
    "Calculate the log likelihood of the LDS models corresponding to the samples\n",
    "`Z` (Vector or Column Matrix) within `mtlds`. `wgt` corr. to importance\n",
    "weights of `Z`. (Under resampling, typically integral valued).\n",
    "\"\"\"\n",
    "function p_log_llh(mtlds::Union{model.MTLDS_g{T,F}, model.MTLDS_ng{T,F}}, \n",
    "        Y::AbstractMatrix{T}, U::AbstractMatrix{T}, Z::AbstractVecOrMat{T}, \n",
    "        wgt::AbstractVector{T}=ones(T, size(Z, 2))) where {T,F}\n",
    "    Ψ = mtlds.nn(Z)\n",
    "    precision = 1 ./ exp.(2*mtlds.logσ)\n",
    "    states = Matrix{T}(undef, size(mtlds,1), size(Z,2))\n",
    "    \n",
    "    llh = map(1:size(Z,2)) do i\n",
    "        lds = model._make_lds_psi(mtlds, Ψ[:,i], mtlds.η_h)\n",
    "        X   = model.state_rollout(lds, U)\n",
    "        Ŷ   = lds.C * X + lds.D * U .+ lds.d;\n",
    "        states[:,i] = Tracker.data(X)[:,end]\n",
    "        - wgt[i] * dot(sum(x->x^2, Y - Ŷ, dims=2), precision)/2\n",
    "    end\n",
    "    return llh, states * (wgt/sum(wgt))\n",
    "end\n",
    "\n",
    "function p_log_prior(Z)\n",
    "    ZZ = Z .* 1/sqrt(prior_var)\n",
    "    exponent = -0.5*sum(ZZ.^2)\n",
    "    return exponent\n",
    "end\n",
    "\n",
    "function p_log_posterior_unnorm(mtlds::Union{model.MTLDS_g{T,F}, model.MTLDS_ng{T,F}}, \n",
    "        Y::AbstractMatrix{T}, U::AbstractMatrix{T}, Z::AbstractVecOrMat{T}) where {T,F}\n",
    "    f_llh = p_log_llh(mtlds, Y, U, Z)\n",
    "    f_prior = p_log_prior(Z)\n",
    "    return f_llh + f_prior\n",
    "end\n",
    "\n",
    "function p_log_posterior_unnorm_beta(mtlds::Union{model.MTLDS_g{T,F}, model.MTLDS_ng{T,F}}, \n",
    "        Y::AbstractMatrix{T}, U::AbstractMatrix{T}, Z::AbstractVecOrMat{T}, beta::T) where {T,F}\n",
    "    f_llh = p_log_llh(mtlds, Y, U, Z)\n",
    "    f_prior = p_log_prior(Z)\n",
    "    return f_prior + beta*f_llh, f_llh + f_prior\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmtlds.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Astable(cmtlds.a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "DhoModel(cmtlds.nn, Flux.param(cmtlds.logσ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to dhoModel code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.change_relative_lr!(cmtlds, 0.10f0)   # reduce sensitivity of chain params. \n",
    "cmtlds_g.η_h .= 0.10f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "function convert_mtlds_to_dhomodel(cmtlds::model.MTLDS_ng)\n",
    "    cmtlds_cp = copy(cmtlds)\n",
    "    model.change_relative_lr!(cmtlds_cp, 1.0f0)\n",
    "    nn = cmtlds_cp.nn\n",
    "    @argcheck length(nn(randn(eltype(cmtlds), 2))) == 11\n",
    "    rm_Bb = setdiff(1:11, [6,7,10,11])\n",
    "    new_ixs = vcat(rm_Bb, 6,7,10,11)\n",
    "    @argcheck length(nn.layers) == 3\n",
    "    @argcheck nn.layers[3] isa Flux.Diagonal\n",
    "    L2 = nn.layers[end-1]\n",
    "    L2 = Dense(L2.W[new_ixs,:], L2.b[new_ixs,:], L2.σ)\n",
    "    L3 = nn.layers[end]\n",
    "    L3 = Flux.Diagonal(L3.α[new_ixs], L3.β[new_ixs])\n",
    "    nn = Chain(nn.layers[1], L2, L3)\n",
    "#     nn.layers[end-1].b = nn.layers[end-1].b[rm_Bb,:]\n",
    "#     nn.layers[end].α = nn.layers[end].α[rm_Bb,:]\n",
    "#     nn.layers[end].β = nn.layers[end].β[rm_Bb,:]\n",
    "    dho_model = DhoModel(nn, Flux.param(cmtlds_cp.logσ))\n",
    "    dho_model_nog = make_untracked(dho_model)\n",
    "    return dho_model_nog\n",
    "end\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "_eps = randn(2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "clds = model.make_lds(cmtlds, f32(_eps), cmtlds.η_h);\n",
    "dho_model_nog = convert_mtlds_to_dhomodel(cmtlds)\n",
    "dho_model = DhoModel(mapleaves(Flux.param, dho_model_nog.nn), Flux.param(dho_model_nog.logσ))\n",
    "dho_model_nog = make_untracked(dho_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(clds(Us32[1])' - dho_model_nog(_eps)'); gcf().set_size_inches(8,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zproposals, w, logp = get_posterior_smp_matrix(dho_model_nog, 200, seq_noise[1:4]; tt=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(get_log_px(cmtlds, seq_noise32[1:4], Us32[1:4], 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2, logp2 = sample_posterior(cmtlds, seq_noise32[1:4], Us32[1:4], 200, f32(Zproposals));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DHO version vs new version. O(1e-6) error expected due to f32.\n",
    "logp - logp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2' - w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.5*sum(x->x^2,(vec(model.make_lds(cmtlds, f32(Zproposals[:,1]), 1.0f0)(Us32[1])) - vec(seq_noise32[1])) / \n",
    "    exp(cmtlds.logσ[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let Δ=(dho_model_nog(Zproposals[:,1]) - seq_noise[1]') ./  exp(Tracker.data(dho_model.logσ[1]));\n",
    "    -0.5*sum(Δ.^2, dims=2)[:]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_log_llh(cmtlds, seq_noise32[1], Us32[1], f32(Zproposals)[:,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_log_llh(dho_model_nog, seq_noise[1], Zproposals[:,1:3]; tt=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function p_log_llh(model::Union{DhoModel, DhoModelUntracked}, Y::Vector, Z::AbstractVecOrMat; tt=TS_LEN)\n",
    "    Ŷ = model(Z, tt=tt)\n",
    "    Δ = (Ŷ .- Y') ./ exp(Tracker.data(model.logσ[1]))\n",
    "    return -0.5*sum(Δ.^2, dims=2)[:]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = subplots(2,2,figsize=(5,5))\n",
    "for i in 1:2, j in 1:2\n",
    "    AxPlot.scatter_alpha(Zproposals[1,:], Zproposals[2,:], w[(i-1)*2+j,:], ax=axs[i,j])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = subplots(2,2,figsize=(5,5))\n",
    "for i in 1:2, j in 1:2\n",
    "    AxPlot.scatter_alpha(Zproposals[1,:], Zproposals[2,:], w2'[1,:], ax=axs[i,j])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================== REDEFINE w/o /10 ============\n",
    "# function B(ψ, d)\n",
    "#     n_skew = Int(d*(d-1)/2)\n",
    "#     ψ_b = ψ[(d+n_skew+1):(d+n_skew+d)]\n",
    "#     return identity.(ψ_b)\n",
    "# end\n",
    "\n",
    "# # ================== ADDL ============\n",
    "# function b(ψ, d)\n",
    "#     n_skew = Int(d*(d-1)/2)\n",
    "#     ψ_b = ψ[(3*d+n_skew+1):(3*d+n_skew+d)]\n",
    "#     return reshape(ψ_b, d)\n",
    "# end\n",
    "\n",
    "# function d_offset(ψ, d)\n",
    "#     n_skew = Int(d*(d-1)/2)\n",
    "#     ψ_d = ψ[(4*d+n_skew+2):(4*d+n_skew+2)]\n",
    "#     return ψ_d\n",
    "# end\n",
    "\n",
    "# function D(ψ, d)\n",
    "#     n_skew = Int(d*(d-1)/2)\n",
    "#     ψ_d = ψ[(4*d+n_skew+1):(4*d+n_skew+1)]\n",
    "#     return ψ_d\n",
    "# end\n",
    "# # ======================================\n",
    "\n",
    "# function _harm_osc2d_state(λ::Vector{T}; tt=TS_LEN)::Matrix{T} where T <: AbstractFloat\n",
    "#     d = 2\n",
    "#     h0 = B(λ, d) * T(1)\n",
    "#     _A = A(λ, d)\n",
    "#     _b = b(λ, d)   # addl\n",
    "#     hidden = lds_internal_notrk{T}(h0 + _b, h0 + _b)   # addl\n",
    "#     function iterate(state)::Vector{T}\n",
    "#         state.x_prev = state.x\n",
    "#         state.x = _A*state.x .+ _b ;    # addl\n",
    "#         state.x_prev\n",
    "#     end\n",
    "    \n",
    "#     lds_state = [iterate(hidden) for t in 1:tt]\n",
    "#     return hcat(lds_state...)\n",
    "# end\n",
    "\n",
    "# function _model_forward(model::Union{DhoModel, DhoModelUntracked}, Z::AbstractMatrix{T}; tt=TS_LEN) where T <: AbstractFloat\n",
    "#     d, M = size(Z, 1), size(Z, 2)   # explicit to ensure always gives 2nd dim (even if none := 1)\n",
    "#     Λ = model.nn(Z)\n",
    "#     Y = map(1:M) do m\n",
    "#         x = _harm_osc2d_state(Λ[:,m]; tt=tt)\n",
    "#         offset = d_offset(Λ[:,m], d)\n",
    "#         ŷ = C(Λ[:,m], d) * x + D(Λ[:,m], d)*hcat(1f0, zeros(Float32, 1, 79)) .+ offset\n",
    "#     end\n",
    "#     return reduce(vcat, Y)\n",
    "# end"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
