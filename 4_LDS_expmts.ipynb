{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin to productionise experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write save/load function for MyLDS.\n",
    "* Think carefully about experiments:\n",
    "    * Train on 70%, validate 10%, test 20%. How?\n",
    "        * Remember that inter-files/intra-files are non-stationary.\n",
    "    * Write loops to extract data and be able to train LDS of various dimension with little user interaction.\n",
    "* How to choose $k$? \n",
    "    * Well we could cross-validate, but I need more Azure cores to do this.\n",
    "    * My belief is that even by $k=30$ we are not really overfitting, especially as resetting every 256.\n",
    "    * Argue that not only does the data exhibit $\\approx 20 d.o.f.$, and we need to store previous frame in state to calculate velocity, if we are even to beat the copy baseline (i.e. need $\\ge$ 20 d.o.f. purely for this). But also linear dynamics from one frame to the next may need more than PCA to permit this. I think 30 is a good place to start.\n",
    "* Build Docker container.\n",
    "* Work out how to mount filesystem on Azure and/or scp out.\n",
    "* Request more Azure compute power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remember that for MTL, we will allow each segment (e.g. 256 length) to be modelled ~ independently.\n",
    "* Need to optimise nograd version of LDS for sampling. My suspicion is that it is not type stable (although it's possible that the matmuls are now dominating a lot more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Revise\n",
    "using LinearAlgebra, Random\n",
    "using StatsBase, Statistics\n",
    "using Distributions, MultivariateStats   # Categorical, P(P)CA\n",
    "using Quaternions    # For manipulating 3D Geometry\n",
    "using MeshCat        # For web visualisation / animation\n",
    "using PyPlot         # Plotting\n",
    "using AxUtil         # Cayley, skew matrices\n",
    "using Flux           # Optimisation\n",
    "using DSP            # convolution / low-pass (MA) filter\n",
    "\n",
    "# small utils libraries\n",
    "using ProgressMeter, Formatting, ArgCheck\n",
    "using BSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_MOCAP_MTDS = \".\" \n",
    "\n",
    "# Data loading and transformation utils\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"io.jl\"))\n",
    "\n",
    "# MeshCat skeleton visualisation tools\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"mocap_viz.jl\"))\n",
    "\n",
    "# Data scaling utils\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"util.jl\"))\n",
    "import .mocaputil: MyStandardScaler, scale_transform, invert\n",
    "import .mocaputil: OutputDifferencer, difference_transform, fit_transform\n",
    "import .mocaputil: no_pos, no_poscp\n",
    "\n",
    "# Models: LDS\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"models.jl\"))\n",
    "import .model: Astable\n",
    "\n",
    "# Table visualisation\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"pretty.jl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "##    CUSTOM WIDELY USED FUNCTIONS\n",
    "function zero_grad!(P) \n",
    "    for x in P\n",
    "        x.grad .= 0\n",
    "    end\n",
    "end\n",
    "\n",
    "rmse(Δ::AbstractArray, scale=size(Δ, 1)) = sqrt(mean(x->x^2, Δ))*scale\n",
    "\n",
    "function rmse(d::mocaputil.DataIterator, m::model.MyLDS_ng)\n",
    "    obj = map(d) do (y, u, new_state)\n",
    "        new_state && (m.h .= zeros(size(m, 1))) \n",
    "        rmse(m(u) - y)\n",
    "    end\n",
    "    m.h .= zeros(size(m, 1))\n",
    "    return dot(obj, mocaputil.weights(d, as_pct=true))\n",
    "end\n",
    "\n",
    "\n",
    "rmse(Ds::Vector{D}, m::model.MyLDS_ng) where {D <: Dict} = rmse(mocaputil.DataIterator(Ds, 1000000), m)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data\n",
    "See `2_Preprocess.ipynb`\n",
    "\n",
    "**Note that in the current harddisk state, `edin_Ys.bson` was created with `include_ftcontact=false`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"../data/mocap/edin-style-transfer/\"\n",
    "files_edin = [joinpath(database, f) for f in readdir(database)];\n",
    "style_name_edin = [x[1] for x in match.(r\"\\.\\./[a-z\\-]+/[a-z\\-]+/[a-z\\-]+/([a-z]+)_.*\", files_edin)];\n",
    "styles = unique(style_name_edin)\n",
    "styles_lkp = [findall(s .== style_name_edin) for s in styles];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Usraw = BSON.load(\"edin_Xs_30fps.bson\")[:Xs];\n",
    "Ysraw = BSON.load(\"edin_Ys_30fps.bson\")[:Ys];\n",
    "\n",
    "standardize_Y = fit(MyStandardScaler, reduce(vcat, Ysraw),  1)\n",
    "standardize_U = fit(MyStandardScaler, reduce(vcat, Usraw),  1)\n",
    "\n",
    "Ys = [scale_transform(standardize_Y, y[2:end, :] ) for y in Ysraw];  # (1-step ahead of u)\n",
    "Us = [scale_transform(standardize_U, u[1:end-1,:]) for u in Usraw];  # (1-step behind y)\n",
    "\n",
    "@assert (let c=cor(Usraw[1][1:end-1, :], Ysraw[1][2:end, :], dims=1); \n",
    "        !isapprox(maximum(abs.(c[.!isnan.(c)])), 1.0); end) \"some input features \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_output_tform(y, i) = invert(standardize_Y, y) |> yhat -> invert(dtforms[i], yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize_Y = fit(model.MyStandardScaler, reduce(vcat, Ysraw),  1)\n",
    "# standardize_U = fit(model.MyStandardScaler, reduce(vcat, Usraw),  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENSE CHECK\n",
    "# check that no bugs in constructing U, Y (i.e. esp that t's align and can predict U --> Y)\n",
    "let c=cor(reduce(vcat, Us) |>f64, reduce(vcat, Ys) |> f64, dims=1)\n",
    "    imshow(c, aspect=\"auto\")\n",
    "    nonan_c = c[.!isnan.(c)]\n",
    "    title(format(\"max (abs) corrcoeff: {:.8f}\", maximum(abs.(nonan_c))))\n",
    "    flush(stdout)\n",
    "    display(findmax(reshape(nonan_c, size(c, 1) - 2, size(c,2))))\n",
    "    printfmtln(\"10th best result {:.5f}\", reverse(sort(nonan_c))[10]) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "expmtdata = mocapio.ExperimentData(Ysraw, [Matrix(y') for y in Ys], \n",
    "    [Matrix(u') for u in Us], styles_lkp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "?mocapio.get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    findin(x, q)\n",
    "Equivalent of `findall` for an elementwise `in` operation. Returns\n",
    "the indices of `[findfirst(xx ∈ q) for xx in x]`.\n",
    "\"\"\"\n",
    "function findin(x, q)\n",
    "    x, q = sort(unique(x)), sort(unique(q))\n",
    "    l = length(q)\n",
    "    out, j = [], 1\n",
    "    for xx in x\n",
    "        while xx > q[j] && j < l\n",
    "            j += 1\n",
    "        end\n",
    "        (xx == q[j]) && push!(out, j)\n",
    "        (j == l) && break\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "results struct.\n",
    "Initialise with `r=init_results()`. Obtain indices via e.g. `validation(r, 2)` will give \n",
    "the 2-step error in the validation set. To set indices use e.g. `set_validation(r, val, 2)`.\n",
    "\"\"\"\n",
    "mutable struct _results{T}\n",
    "    data::Matrix{T}\n",
    "end\n",
    "lkah = [1,2,4,8,14,25]\n",
    "init_results() = _results(ones(3, 6)*NaN)\n",
    "results_struct(model_names) = Dict(m=>init_results() for m in model_names)\n",
    "training(s::_results, i=lkah, ix=findin(i,lkah)) = s.data[1,ix]'\n",
    "validation(s::_results, i=lkah, ix=findin(i,lkah)) = s.data[2,ix]'\n",
    "testing(s::_results, i=lkah, ix=findin(i,lkah)) = s.data[3,ix]'\n",
    "set_training(s::_results, val, i=lkah, ix=findin(i,lkah)) = (s.data[1,ix] = val);\n",
    "set_validation(s::_results, val, i=lkah, ix=findin(i,lkah)) = (s.data[2,ix] = val);\n",
    "set_testing(s::_results, val, i=lkah, ix=findin(i,lkah)) = (s.data[3,ix] = val);\n",
    "\n",
    "\n",
    "\n",
    "aggregate_generic(v::Array, f::Function, model::Symbol, g::Function; args...) = \n",
    "    g(reduce(vcat, [f(v[i][model]) for i in 1:8]); args...)\n",
    "\n",
    "aggregate_mean(v::Array, f::Function, model::Symbol) = aggregate_generic(v, f, model, mean; dims=1)\n",
    "aggregate_std(v::Array, f::Function, model::Symbol) = aggregate_generic(v, f, model, std; dims=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [:copy, :LR, :LDS2init, :LDS20init, :LDS40init, :LDS20smp, :LDS40smp]\n",
    "results_stl = [results_struct(model_types) for i in 1:8]\n",
    "results_pool = [results_struct(model_types) for i in 1:8];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSTL, validSTL, testSTL = mocapio.get_data(expmtdata, 1, :split, :pool; concat=true,\n",
    "simplify=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy prev frame model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL TYPE: COPY\n",
    "for style_ix = 1:8\n",
    "    # Get training set for STL and pooled models.\n",
    "    trainSTL, validSTL, testSTL = mocapio.get_data(expmtdata, style_ix, :split, :stl;\n",
    "                                    concat=true, simplify=true)\n",
    "    trainPool, validPool, testPool = mocapio.get_data(expmtdata, style_ix, :split, \n",
    "                                        :pooled, concat=true, simplify=true)\n",
    "    cN_STL = size(trainSTL[:Y], 2);\n",
    "    \n",
    "    # Create \"Copy\" model\n",
    "    model_copy(test_set, k_step=1) = test_set[:,1:end-k_step]\n",
    "    \n",
    "    for (settr, data) in zip([set_training, set_validation, set_testing], [trainSTL, validSTL, testSTL])\n",
    "        results = [rmse(model_copy(data[:Y], k) - data[:Y][:, 1+k:end]) for k in lkah]\n",
    "        settr(results_stl[style_ix][:copy], results);\n",
    "    end\n",
    "    \n",
    "    for (settr, data) in zip([set_training, set_validation], [trainPool, validPool])\n",
    "        results = [rmse(model_copy(data[:Y], k) - data[:Y][:, 1+k:end]) for k in lkah]\n",
    "        settr(results_pool[style_ix][:copy], results);\n",
    "    end\n",
    "    results = [rmse(model_copy(testPool[:Y], k) - testPool[:Y][:, 1+k:end]) for k in lkah]\n",
    "    set_testing(results_pool[style_ix][:copy], results);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL TYPE: LINEAR REGRESSION\n",
    "\n",
    "@showprogress for style_ix = 1:8\n",
    "    # Get training set for STL and pooled models.\n",
    "    trainSTL, validSTL, testSTL = mocapio.get_data(expmtdata, style_ix, :split, :stl,\n",
    "                                    concat=true, simplify=true)\n",
    "    trainPool, validPool, testPool = mocapio.get_data(expmtdata, style_ix, :split, :pooled,\n",
    "                                        concat=true, simplify=true)\n",
    "    cN_STL = size(trainSTL[:Y], 2);\n",
    "    \n",
    "    # Create model\n",
    "    function model_LR(train_set) \n",
    "        Dd = train_set[:Y] / [train_set[:U]; ones(1, size(train_set[:U], 2))]\n",
    "        clds = model.init_LDS_spectral(validSTL[:Y], validSTL[:U], 1, max_iter=1);\n",
    "        clds.C .= 0\n",
    "        clds.B .= 0\n",
    "        clds.b .= 0\n",
    "        clds.D .= Dd[:,1:end-1]\n",
    "        clds.d .= Dd[:,end]\n",
    "        lr(test, k) =  model.kstep_predict(clds, test[:U], test[:Y], standardize_Y, standardize_U, 10, k)\n",
    "        return lr\n",
    "    end\n",
    "    \n",
    "    # Train model\n",
    "    cmodel = model_LR(trainSTL)\n",
    "    \n",
    "    # Evaluate on Train/Validation/Test\n",
    "    for (settr, data) in zip([set_training, set_validation, set_testing], [trainSTL, validSTL, testSTL])\n",
    "        results = [rmse(cmodel(data, k) - data[:Y][:, k:end]) for k in lkah]\n",
    "        settr(results_stl[style_ix][:LR], results);\n",
    "    end\n",
    "    \n",
    "    ################### POOLED DATA ##############################\n",
    "\n",
    "    # Train model\n",
    "    cmodel = model_LR(trainPool)\n",
    "    \n",
    "    # Evaluate on Train/Validation set\n",
    "    for (settr, data) in zip([set_training, set_validation], [trainPool, validPool])\n",
    "        results = [rmse(cmodel(data, k) - data[:Y][:, k:end]) for k in lkah]\n",
    "        settr(results_pool[style_ix][:LR], results);\n",
    "    end\n",
    "    \n",
    "    # Evaluate on Test Set\n",
    "    results = [rmse(cmodel(testPool, k) - testPool[:Y][:, k:end]) for k in lkah]\n",
    "    set_testing(results_pool[style_ix][:LR], results);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty.table([aggregate_mean(results_pool, training, m)/8 for m in [:copy, :LR]],\n",
    "             [aggregate_mean(results_pool, testing, m)/8 for m in [:copy, :LR]]; \n",
    "    header_row=lkah, header_col=[\"Copy\", \"Linear Reg\"], title=[\"Train\", \"Test\"], dp=2, header=\"Average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty.table([aggregate_std(results_pool, training, m)/8 for m in [:copy, :LR]],\n",
    "             [aggregate_std(results_pool, testing, m)/8 for m in [:copy, :LR]]; \n",
    "        header_row=lkah, header_col=[\"Copy\", \"Linear Reg\"], title=[\"Train\", \"Test\"], dp=2, header=\"Standard Dev.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coord Desc LDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = :LDS20init\n",
    "\n",
    "@showprogress for style_ix = 1:8\n",
    "    # Get training set for STL and pooled models.\n",
    "    trainSTL, validSTL, testSTL = mocapio.get_data(expmtdata, style_ix, :split, :stl,\n",
    "                                    concat=true, simplify=true)\n",
    "    trainPool, validPool, testPool = mocapio.get_data(expmtdata, style_ix, :split, :pooled,\n",
    "                                    concat=true, simplify=true)\n",
    "    cN_STL = size(trainSTL[:Y], 2);\n",
    "    \n",
    "    # Train model\n",
    "    clds = model.init_LDS_spectral(trainSTL[:Y], trainSTL[:U], 20);\n",
    "    cmodel(test, k) = model.kstep_predict(clds, test[:U], test[:Y], standardize_Y, standardize_U, 10, k)\n",
    "    \n",
    "    # Evaluate on Train/Validation/Test\n",
    "    for (settr, data) in zip([set_training, set_validation, set_testing], [trainSTL, validSTL, testSTL])\n",
    "        results = [rmse(cmodel(data, k) - data[:Y][:, k:end]) for k in lkah]\n",
    "        settr(results_stl[style_ix][model_name], results);\n",
    "    end\n",
    "    \n",
    "    ################### POOLED DATA ##############################\n",
    "\n",
    "    # Train model\n",
    "    clds = model.init_LDS_spectral(trainPool[:Y], trainPool[:U], 20);\n",
    "    cmodel(test, k) = model.kstep_predict(clds, test[:U], test[:Y], standardize_Y, standardize_U, 10, k)\n",
    "    \n",
    "    # Evaluate on Train/Validation/Test set\n",
    "    for (settr, data) in zip([set_training, set_validation, set_testing], [trainPool, validPool, testPool])\n",
    "        results = [rmse(cmodel(data, k) - data[:Y][:, k:end]) for k in lkah]\n",
    "        settr(results_pool[style_ix][model_name], results);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = :LDS40init\n",
    "\n",
    "@showprogress for style_ix = 1:8\n",
    "    # Get training set for STL and pooled models.\n",
    "    trainSTL, validSTL, testSTL = mocapio.get_data(expmtdata, style_ix, :split, :stl,\n",
    "                                    concat=true, simplify=true)\n",
    "    trainPool, validPool, testPool = mocapio.get_data(expmtdata, style_ix, :split, :pooled,\n",
    "                                    concat=true, simplify=true)\n",
    "    cN_STL = size(trainSTL[:Y], 2);\n",
    "    \n",
    "    # Train model\n",
    "    clds = model.init_LDS_spectral(trainSTL[:Y], trainSTL[:U], 40);\n",
    "    cmodel(test, k) = model.kstep_predict(clds, test[:U], test[:Y], standardize_Y, standardize_U, 10, k)\n",
    "    \n",
    "    # Evaluate on Train/Validation/Test\n",
    "    for (settr, data) in zip([set_training, set_validation, set_testing], [trainSTL, validSTL, testSTL])\n",
    "        results = [rmse(cmodel(data, k) - data[:Y][:, k:end]) for k in lkah]\n",
    "        settr(results_stl[style_ix][model_name], results);\n",
    "    end\n",
    "    \n",
    "    ################### POOLED DATA ##############################\n",
    "\n",
    "    # Train model\n",
    "    clds = model.init_LDS_spectral(trainPool[:Y], trainPool[:U], 40);\n",
    "    cmodel(test, k) = model.kstep_predict(clds, test[:U], test[:Y], standardize_Y, standardize_U, 10, k)\n",
    "    \n",
    "    # Evaluate on Train/Validation/Test set\n",
    "    for (settr, data) in zip([set_training, set_validation, set_testing], [trainPool, validPool, testPool])\n",
    "        results = [rmse(cmodel(data, k) - data[:Y][:, k:end]) for k in lkah]\n",
    "        settr(results_pool[style_ix][model_name], results);\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty.table([aggregate_mean(results_stl, training, m)/8 for m in [:copy, :LR, :LDS20init, :LDS40init]],\n",
    "             [aggregate_mean(results_stl, testing, m)/8 for m in [:copy, :LR, :LDS20init, :LDS40init]]; \n",
    "    header_row=lkah, header_col=[\"Copy\", \"Linear Reg\", \"LDS20init\", \"LDS40init\"], title=[\"Train\", \"Test\"], \n",
    "    dp=2, header=\"Average -- STL Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty.table([aggregate_mean(results_pool, training, m)/8 for m in [:copy, :LR, :LDS20init, :LDS40init]],\n",
    "             [aggregate_mean(results_pool, testing, m)/8 for m in [:copy, :LR, :LDS20init, :LDS40init]]; \n",
    "    header_row=lkah, header_col=[\"Copy\", \"Linear Reg\", \"LDS20init\", \"LDS40init\"], title=[\"Train\", \"Test\"], \n",
    "    dp=2, header=\"Average -- Pooled Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bits and pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_types = [:LDS40init_t2]\n",
    "# for i in 1:8\n",
    "#     for m in model_types\n",
    "#         results_stl[i][m] = init_results()\n",
    "#         results_pool[i][m] = init_results()\n",
    "#     end\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an STL model initialised from Pooled\n",
    "\n",
    "* Looking at `angry` style (`ix=1`).\n",
    "* L2 error of STL is 0.84 in training, 1.65 in test. Note however the end of the seq is harder.\n",
    "* L2 test error of Pooled model on this style is 1.71. Clearly this is worse than STL, especially since the test result of STL is only of the test part: the hardest bit.\n",
    "* So the gap is something like 1.71 (pooled) vs ~0.80 (STL). (not apples to apples due to test set of STL.)\n",
    "\n",
    "From initialisation using the pooled model, **can SGD get close to the STL results, when training on the STL training set?**.\n",
    "\n",
    "==> *The answer is that it can do **fairly** well, getting to ~ 1.00 on the STL test set. If the exact regression step can be used (note it cannot obviously be done for MTL), then we get close to 0.90.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcat(training(results_pool[1][:LDS20init]), testing(results_pool[1][:LDS20init]))/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcat(training(results_stl[1][:LDS20init]), validation(results_stl[1][:LDS20init]))/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training set for STL and pooled models.\n",
    "style_ix = 1\n",
    "k = 20\n",
    "\n",
    "trainPool, validPool, testPool = mocapio.get_data(expmtdata, style_ix, :split, :pooled, concat=true, simplify=true)\n",
    "\n",
    "# Train model\n",
    "clds_orig = model.init_LDS_spectral(trainPool[:Y], trainPool[:U], k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSTL, validSTL, testSTL = mocapio.get_data(expmtdata, style_ix, :split, :stl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clds_g = model.make_grad(model.init_LDS_spectral(cYs, cUT, k))\n",
    "clds_g = model.make_grad(clds_orig)\n",
    "clds   = model.make_nograd(clds_g)   # MUST DO THIS SECOND, (Flux.param takes copy)\n",
    "\n",
    "# clds.a .= vcat(ones(k)*2.3, zeros(Int(k*(k-1)/2)))\n",
    "\n",
    "opt = ADAM(1e-4)\n",
    "opt_hidden = ADAM(0.7e-5)\n",
    "\n",
    "ps_hidden = Flux.params(clds_g.a, clds_g.B, clds_g.b)\n",
    "ps_observ = Flux.params(clds_g.C, clds_g.D, clds_g.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(results_pool[1][:LDS20init])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_optimal_obs_params(clds, trainSTL[:Y], trainSTL[:U]);   # don't want to use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(testPool, clds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 192\n",
    "min_size = 50\n",
    "@time h = begin\n",
    "    n_epochs = 200\n",
    "    # max batch length ==> will be variable as starting position is stochastic.\n",
    "    trainIter = mocaputil.DataIterator(testPool, batch_size, min_size=min_size)\n",
    "    W = mocaputil.weights(trainIter; as_pct=false) ./ batch_size\n",
    "    nB = length(trainIter)\n",
    "    history = ones(n_epochs*nB) * NaN\n",
    "    \n",
    "    for ee in 1:n_epochs\n",
    "#         bix = rand(1:64)   # begin at index (in each mocap file)\n",
    "#         trainIter = mocaputil.DataIterator(trainSTL, batch_size, min_size=min_size, start=bix);\n",
    "        if ee == 1\n",
    "            opt.eta, opt_hidden.eta = 0., 0.\n",
    "        elseif ee == 3\n",
    "            opt.eta = 5e-4 / 10 * 0.1 * 0.3 * 10 \n",
    "            opt_hidden.eta =  0.02e-4 * 10 * 0.005 * 1\n",
    "        elseif ee % 100 == 0\n",
    "            printfmtln(\"100 epochs\")\n",
    "        end\n",
    "        \n",
    "        for (ii, (_cY, _cU, h0)) in enumerate(trainIter)\n",
    "            h0 && (clds_g.h.data .= zeros(size(clds_g, 1)))   # reset state?\n",
    "            X̂ = model.state_rollout(clds_g, _cU); \n",
    "            Yhat = clds_g.C * X̂ + clds_g.D * _cU .+ clds_g.d;\n",
    "            obj = mean(x->x^2, _cY - Yhat)*64^2 * W[ii]\n",
    "            history[(ee-1)*nB + ii] = obj.data\n",
    "            Tracker.back!(obj)\n",
    "            \n",
    "            for p in ps_hidden\n",
    "                Tracker.update!(opt_hidden, p, -Tracker.grad(p))\n",
    "            end\n",
    "            for p in ps_observ\n",
    "                Tracker.update!(opt, p, -Tracker.grad(p))\n",
    "            end\n",
    "            clds_g.h.data .= X̂.data[:,end]\n",
    "        end\n",
    "        println(sqrt(mean(history[(1:nB) .+ nB*(ee-1)])))\n",
    "    end\n",
    "    history\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(sqrt.(conv(h, Windows.rect(nB))[nB:end-nB+1]/nB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task modelling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(DIR_MOCAP_MTDS, \"models.jl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_ix = 1\n",
    "d_state = 4\n",
    "# Train model\n",
    "trainAll = mocapio.get_data(expmtdata, style_ix, :all, :pooled, concat=true, simplify=true)\n",
    "clds_orig = model.init_LDS_spectral(trainAll[:Y], trainAll[:U], d_state);\n",
    "trainAll = nothing  # fairly large (~60 MB), don't want a multiplicity of these kicking around memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2                 # dimension of manifold\n",
    "d_nn = 200            # \"complexity\" of manifold\n",
    "d_subspace = 40       # dim of subspace (⊆ parameter space) containg the manifold\n",
    "\n",
    "d_par = length(model.get_pars(clds_orig))\n",
    "nn = Chain(Dense(k, d_nn, tanh), Dense(d_nn, d_subspace, σ), \n",
    "    Dense(d_subspace, d_par,identity, initW = ((dims...)->Flux.glorot_uniform(dims...)*0.1)),\n",
    "    Flux.Diagonal(d_par))\n",
    "nn = Chain(Dense(k, d_par, identity), Flux.Diagonal(d_par))\n",
    "clogσ = repeat([-1.5f0], size(clds_orig, 2))\n",
    "\n",
    "cmtlds_g = model.mtldsg_from_lds(clds_orig, nn, clogσ, 0.1f0);\n",
    "cmtlds = model.make_nograd(cmtlds_g);\n",
    "# model.change_relative_lr!(cmtlds, 0.001f0)   # reduce sensitivity of chain params. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(trainSTL, model.make_lds(cmtlds, randn(Float32, k), 0.1f0))/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(trainSTL, model.make_lds(cmtlds, _Z[1:k,1], 0.1f0))/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "For sampling, we can happily perform the following:\n",
    "\n",
    "```julia\n",
    "    ϵ = randn(Float32, k, 200)\n",
    "    for i in 1:200\n",
    "        lds = model.make_lds(cmtlds, ϵ[:,i])\n",
    "        smp = lds(U)\n",
    "        ...\n",
    "    end\n",
    "```\n",
    "\n",
    "where we can use all the nice utility functions to create an LDS from the MTLDS and sample accordingly using the fast sampling from LDS objects.\n",
    "\n",
    "However, for optimisation (once samples have been chosen), it is highly suboptimal to backprop through the (large) FFNN one sample at a time. Instead we should probably do the following:\n",
    "\n",
    "```julia\n",
    "    Z # from posterior sampling over the ϵ in the above stage\n",
    "    Ψ = cmtlds.nn(Z)\n",
    "    for i in 1:size(Ψ,2)\n",
    "        lds = model._make_lds_psi(cmtlds, Ψ[:,i])\n",
    "        obj += error(lds(U), Y)\n",
    "        ...\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions\n",
    "\n",
    "* **make_lds**: `model.make_lds(cmtlds, randn(Float32, 3), 0.1f0)`. Make the LDS object corresponding to the value of `z` (here a `randn`) acting on `cmtlds`. The 3rd argument specifies the *relative* learning rate of the latent chain parameters. (Irrelevant if not performing optimisation.)\n",
    "* **rmse**: `rmse(dataDicts, lds::MyLDS_ng)`: calculate the sum of RMSE over all data in `dataDicts`.\n",
    "* **p_log_llh**: `p_log_llh(cmtlds, randn(Float32, 64, 200), randn(Float32, 121, 200)*0.1f0, Z)`. Calculate the log likelihood of the LDS models corresponding to the samples `Z` (can also supply importance weights in final argument (not shown)). Note that this value $\\propto$ the length of data it is given.\n",
    "* **sample_posterior**: `sample_posterior(cmtlds, trainSTL[1][:Y], trainSTL[1][:U], 100)`. Returns weights, the estimated log_px, and the samples used to calculate them. Instead of generating samples, the function can also be given them as a 5th argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample_forward(mtlds::model.MTLDS_ng{T,F}, U::AbstractArray{T}, M::Int, ϵ::AbstractMatrix{T}) where {T, F}\n",
    "    return [model.make_lds(mtlds, view(ϵ, :, i), mtlds.η_h)(U) for i in 1:M]\n",
    "end\n",
    "\n",
    "function sample_forward(mtlds::model.MTLDS_ng{T,F}, U::AbstractArray{T}, M::Int) where {T, F}\n",
    "    k = size(mtlds.nn.layers[1].W, 2)\n",
    "    ϵ = convert(Array{T}, AxUtil.Random.sobol_gaussian(M, k)')\n",
    "    return (sample_forward(mtlds, U, M, ϵ)..., ϵ)\n",
    "end\n",
    "\n",
    "\n",
    "@inline _gauss_lognormconst(logσ::Vector, tt) = 0.5*tt*length(logσ)*log(2π) + 0.5*tt*sum(2*logσ)\n",
    "\n",
    "#= slightly faster version for when a single Y (can do subtraction in-place).\n",
    "   This is called much more frequently during training (i.e. before every couple of batches) =#\n",
    "function sample_posterior(mtlds::model.MTLDS_ng{T,F}, Y::AbstractArray{T}, \n",
    "            U::AbstractArray{T}, M::Int, ϵ::AbstractArray{T}) where {T, F}\n",
    "    \n",
    "    Ŷ = sample_forward(mtlds, U, M, ϵ)\n",
    "    \n",
    "    # Calculate density of each sample Ŷ\n",
    "    Δnorm = Vector{T}(undef, M)\n",
    "    precision = 1 ./ exp.(2*mtlds.logσ)\n",
    "    for i in 1:M\n",
    "        @views Ŷ[i] .-= Y\n",
    "        Δnorm[i] = dot(sum(x->x^2, Ŷ[i], dims=2), precision)\n",
    "    end\n",
    "\n",
    "    # Elementwise logpdf ==> single logpdf and importance weights\n",
    "    el_lpdf = -0.5 * Δnorm .- _gauss_lognormconst(mtlds.logσ, size(Y,2))\n",
    "    W, lgpdf = AxUtil.Math.softmax_lse!(reshape(el_lpdf, :, 1))   # softmax AND logsumexp\n",
    "    return W, lgpdf[1] .- log(M)  # ϵ, W, log(1/M sum_j exp(log_j))\n",
    "end\n",
    "\n",
    "function sample_posterior(mtlds::model.MTLDS_ng{T,F}, Y::AbstractArray{T}, \n",
    "            U::AbstractArray{T}, M::Int) where {T, F}\n",
    "    k = size(mtlds.nn.layers[1].W, 2)\n",
    "    ϵ = convert(Array{T}, AxUtil.Random.sobol_gaussian(M, k)')\n",
    "    return (sample_posterior(mtlds, Y, U, M, ϵ)..., ϵ)\n",
    "end\n",
    "\n",
    "\n",
    "# slower version for *series of matrices* for Y (can no longer do in-place) \n",
    "function sample_posterior(mtlds::model.MTLDS_ng{T,F}, Ys::AbstractArray{MT}, \n",
    "            Us::AbstractArray{MT}, M::Int) where {T, F, MT <: Matrix}\n",
    "    \n",
    "    k = size(mtlds.nn.layers[1].W, 2)\n",
    "    \n",
    "    # common r.v.s\n",
    "    ϵ = convert(Array{T}, AxUtil.Random.sobol_gaussian(M, k)')\n",
    "    W, lgpdf = sample_posterior(mtlds, Ys, Us, M, ϵ)\n",
    "    \n",
    "    return Matrix(W'), lgpdf, ϵ  # W, log(1/M sum_j exp(log_j)), ϵ\n",
    "end\n",
    "\n",
    "function sample_posterior(mtlds::model.MTLDS_ng{T,F}, Ys::AbstractArray{MT}, \n",
    "            Us::AbstractArray{MT}, M::Int, ϵ::AbstractArray{T}) where {T, F, MT <: Matrix}\n",
    "    \n",
    "    N = length(Ys)\n",
    "    W = Matrix{T}(undef, N, M)\n",
    "    lgpdf = Vector{T}(undef, N)\n",
    "    \n",
    "    for nn in 1:N\n",
    "        _W, _lp = sample_posterior(mtlds, Ys[nn], Us[nn], M, ϵ)\n",
    "        W[nn,:] = _W[:]\n",
    "        lgpdf[nn] = _lp\n",
    "    end\n",
    "    \n",
    "    return Matrix(W'), lgpdf  # W, log(1/M sum_j exp(log_j)), ϵ\n",
    "end\n",
    "\n",
    "function get_log_px(model::model.MTLDS_ng{T,F}, Y::AbstractArray, U::AbstractArray, m::Int) where {T,F}\n",
    "    sample_posterior(model, Y, U, m)[2]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "const prior_var = 1.0\n",
    "\"\"\"\n",
    "    p_log_llh(mtlds, Y, U, Z, wgt=ones(T, size(Z, 2)))\n",
    "Calculate the log likelihood of the LDS models corresponding to the samples\n",
    "`Z` (Vector or Column Matrix) within `mtlds`. `wgt` corr. to importance\n",
    "weights of `Z`. (Under resampling, typically integral valued).\n",
    "\"\"\"\n",
    "function p_log_llh(mtlds::Union{model.MTLDS_g{T,F}, model.MTLDS_ng{T,F}}, \n",
    "        Y::AbstractMatrix{T}, U::AbstractMatrix{T}, Z::AbstractVecOrMat{T}, \n",
    "        wgt::AbstractVector{T}=ones(T, size(Z, 2))) where {T,F}\n",
    "    Ψ = mtlds.nn(Z)\n",
    "    precision = 1 ./ exp.(2*mtlds.logσ)\n",
    "    states = Matrix{T}(undef, size(mtlds,1), size(Z,2))\n",
    "    \n",
    "    llh = map(1:size(Z,2)) do i\n",
    "        lds = model._make_lds_psi(mtlds, Ψ[:,i], mtlds.η_h)\n",
    "        X   = model.state_rollout(lds, U)\n",
    "        Ŷ   = lds.C * X + lds.D * U .+ lds.d;\n",
    "        states[:,i] = Tracker.data(X)[:,end]\n",
    "        - wgt[i] * dot(sum(x->x^2, Y - Ŷ, dims=2), precision)/2\n",
    "    end\n",
    "    return llh, states * (wgt/sum(wgt))\n",
    "end\n",
    "\n",
    "function p_log_prior(Z)\n",
    "    ZZ = Z .* 1/sqrt(prior_var)\n",
    "    exponent = -0.5*sum(ZZ.^2)\n",
    "    return exponent\n",
    "end\n",
    "\n",
    "function p_log_posterior_unnorm(mtlds::Union{model.MTLDS_g{T,F}, model.MTLDS_ng{T,F}}, \n",
    "        Y::AbstractMatrix{T}, U::AbstractMatrix{T}, Z::AbstractVecOrMat{T}) where {T,F}\n",
    "    f_llh = p_log_llh(mtlds, Y, U, Z)\n",
    "    f_prior = p_log_prior(Z)\n",
    "    return f_llh + f_prior\n",
    "end\n",
    "\n",
    "function p_log_posterior_unnorm_beta(mtlds::Union{model.MTLDS_g{T,F}, model.MTLDS_ng{T,F}}, \n",
    "        Y::AbstractMatrix{T}, U::AbstractMatrix{T}, Z::AbstractVecOrMat{T}, beta::T) where {T,F}\n",
    "    f_llh = p_log_llh(mtlds, Y, U, Z)\n",
    "    f_prior = p_log_prior(Z)\n",
    "    return f_prior + beta*f_llh, f_llh + f_prior\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= CALCULATING LOGPX IS REALLY EXPENSIVE!! ============================\n",
    "tmp = sample_posterior(cmtlds, [trainPool[i][:Y][:,1:680] for i in 1:7], \n",
    "    [trainPool[i][:U][:,1:680] for i in 1:7], 500)   # 8 secs for 10% of (*nonstationary!*) training set (M=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DATA\n",
    "trainSTL, validSTL, testSTL = mocapio.get_data(expmtdata, style_ix, :split, :stl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2                 # dimension of manifold\n",
    "d_nn = 200            # \"complexity\" of manifold\n",
    "d_subspace = 40       # dim of subspace (⊆ parameter space) containg the manifold\n",
    "\n",
    "d_par = length(model.get_pars(clds_orig))\n",
    "nn = Chain(Dense(k, d_nn, tanh), Dense(d_nn, d_subspace, σ), \n",
    "    Dense(d_subspace, d_par,identity, initW = ((dims...)->Flux.glorot_uniform(dims...)*0.1)),\n",
    "    Flux.Diagonal(d_par))\n",
    "# nn = Chain(Dense(k, d_par, identity), Flux.Diagonal(d_par))\n",
    "clogσ = repeat([-1.5f0], size(clds_orig, 2))\n",
    "\n",
    "cmtlds_g = model.mtldsg_from_lds(clds_orig, nn, clogσ, 0.1f0);\n",
    "cmtlds = model.make_nograd(cmtlds_g);\n",
    "# model.change_relative_lr!(cmtlds, 0.001f0)   # reduce sensitivity of chain params. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = ADAM(8e-4)\n",
    "pars = model.pars(cmtlds_g);\n",
    "\n",
    "# --- Training parameters ---\n",
    "T = eltype(cmtlds)\n",
    "n_epochs   = 2\n",
    "m_proposal = 200\n",
    "m_bprop    = 3\n",
    "batch_sz   = 192 # ???\n",
    "prior_lsigma = MvNormal(zeros(T, 64), Diagonal(100*ones(T, 64)))\n",
    "# --- /end ---\n",
    "\n",
    "history = zeros(T, n_epochs, 2)\n",
    "trainData = mocaputil.DataIterator(trainSTL, batch_sz, min_size=50)\n",
    "Ws = mocaputil.weights(trainData, as_pct=false)\n",
    "\n",
    "# for ee in 1:n_epochs\n",
    "\n",
    "total_len  = sum(Ws)\n",
    "epoch_loss = zero(T)\n",
    "recon_loss = zero(T)\n",
    "logp = zero(T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmtlds = model.make_nograd(cmtlds_g);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmtlds.nn.layers[1].W[1,1] = -0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmtlds_g.nn.layers[1].W[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [],
   "source": [
    "_Z = randn(Float32, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_posterior(cmtlds, validSTL[1][:Y], validSTL[1][:U], 1, _Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_log_llh(cmtlds, validSTL[1][:Y], validSTL[1][:U], randn(Float32, 3))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Yb, Ub, h0) = first(trainData);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 && model.zero_state!(clds_g)\n",
    "Tb = size(Yb, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_log_px(cmtlds, trainSTL[4][:Y], trainSTL[4][:U], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmtlds.nn(randn(Float32, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(validSTL, model.make_lds(cmtlds, randn(Float32, 3), cmtlds.η_h))/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.make_lds(cmtlds, randn(Float32, 3), cmtlds.η_h)(validSTL[1][:U])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "_eps = randn(Float32, 3, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_posterior(cmtlds, validSTL[1][:Y][:,1:192], validSTL[1][:U][:,1:192], 20, _eps)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "using AxPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(_eps[1,:], _eps[3,:], alpha=0.1)\n",
    "AxPlot.scatter_alpha(_eps[1,:], _eps[3,:], \n",
    "    f32(sample_posterior(cmtlds, validSTL[1][:Y], validSTL[1][:U], 20, _eps)[1])[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(_eps[1,:], _eps[2,:], alpha=0.1)\n",
    "AxPlot.scatter_alpha(_eps[1,:], _eps[2,:], \n",
    "    f32(sample_posterior(cmtlds, validSTL[1][:Y], validSTL[1][:U], 20, _eps)[1])[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.make_lds(cmtlds, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(vcat([vec(p.grad) for p in pars]...))\n",
    "gca().set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.eta *= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.eta /=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5*(let δ=(cmtlds.logσ - prior_lsigma.μ); δ' * (inv(prior_lsigma.Σ) * δ); end) / Tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmtlds.logσ   #.-= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5*(let δ=(cmtlds.logσ - prior_lsigma.μ); δ' * (inv(prior_lsigma.Σ) * δ); end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open questions\n",
    "\n",
    "* Why does STL not appear to converge to even the non-MTL optimum?\n",
    "    * Why is it still attempting to increase even with the dynamics grad ~=0?\n",
    "* Why does logσ go in the wrong direction?\n",
    "\n",
    "\n",
    "\n",
    "* *(Bonus: the low rank matrix thing of two linear layers stacked at the end didn't appear to work well. What is going on here? When does the sigmoid nonlinearity help? Is it a conditioning problem?)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.change_relative_lr!(cmtlds, 0.1f0) \n",
    "cmtlds_g.η_h .= 0.1f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lp = [get_log_px(cmtlds, trainSTL[i][:Y], trainSTL[i][:U], 200) for i in 1:length(trainSTL)]\n",
    "sum(_lp)/sum([size(trainSTL[i][:Y],2) for i in 1:length(trainSTL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- Training parameters ---\n",
    "T = eltype(cmtlds)\n",
    "n_epochs   = 100\n",
    "m_proposal = 300\n",
    "m_bprop    = 2\n",
    "batch_sz   = 192 # ???\n",
    "# --- /end ---\n",
    "\n",
    "history = zeros(T, n_epochs, 2)\n",
    "trainData = mocaputil.DataIterator(trainSTL, batch_sz, min_size=50)\n",
    "Ws = mocaputil.weights(trainData, as_pct=false)\n",
    "\n",
    "logσ_prior = (-1.3, 0.05)\n",
    "prior_lsigma = MvNormal(ones(T, 64)*logσ_prior[1], Diagonal(logσ_prior[2]*ones(T, 64)))\n",
    "\n",
    "opt.eta = 0.2e-5    # 3.8e-5  0.8e-5\n",
    "for ee in 1:n_epochs\n",
    "    total_len  = sum(Ws)\n",
    "    epoch_loss = zero(T)\n",
    "    recon_loss = zero(T)\n",
    "    logp = zero(T)\n",
    "\n",
    "\n",
    "    for (Yb, Ub, h0) in trainData\n",
    "        h0 && model.zero_state!(cmtlds_g)\n",
    "        Tb = size(Yb, 2)      # not constant\n",
    "\n",
    "        # ====== Generate approximate posterior samples ==============\n",
    "        # ---> importance sample w forward sim.\n",
    "        w, logp_s, Zproposal = sample_posterior(cmtlds, Yb, Ub, m_proposal)\n",
    "        logp += sum(logp_s)\n",
    "\n",
    "        # ---> resample and aggregate duplicates\n",
    "        smps = AxUtil.Random.multinomial_indices_linear(m_bprop, w[:])\n",
    "        # often duplicates (e.g. m_bprop of same sample), esp nr beginning. Aggregate to improve efficiency.\n",
    "        smps, smp_wgt = countmap(smps) |> x-> (collect(keys(x)), collect(values(x)))\n",
    "        Zs_post = Zproposal[:, smps]\n",
    "        Zs_wgt  = T.(smp_wgt) / m_bprop\n",
    "#         logp += 0\n",
    "#         Zs_post = _Z[:,1:1]\n",
    "#         Zs_wgt = [T(1.0)]\n",
    "\n",
    "        # ===== AutoDiff-aware pass through posterior-integrated likelihood =====\n",
    "        # ---> neg. log likelihood / reconstruction\n",
    "        llh, state = p_log_llh(cmtlds_g, Yb, Ub, Zs_post, Zs_wgt)\n",
    "        cmtlds.h .= state   # update state acc. to forward sim in p_log_llh.\n",
    "#         println(llh.data)\n",
    "\n",
    "        # ---> Update loss functions\n",
    "        loss   = - sum(llh)  # *decrease* *negative* llh. (Don't normalise for data length.)\n",
    "        recon_loss += -loss.data - Tb*(sum(cmtlds.logσ) + size(cmtlds, 2)*log(2π)/2)\n",
    "\n",
    "        # .... Likelihood normalising constant  (# not *n* Tb sum(logσ) as we have normalised by n already)\n",
    "        loss += Tb * sum(cmtlds.logσ)\n",
    "\n",
    "        # .... Log Normal prior on log sigma\n",
    "        loss += 0.5 * sum(x->x^2, (cmtlds.logσ .- logσ_prior[1]))/logσ_prior[2]\n",
    "#         loss += 0.5*(let δ=(cmtlds.logσ - prior_lsigma.μ); δ' * (inv(prior_lsigma.Σ) * δ); end)\n",
    "\n",
    "        # .... Regularisation of MLP weights\n",
    "        # careful of regularizing layer 3 (due to lr rescale)\n",
    "#         loss += 1e-3*sum(abs, cmtlds.nn.layers[1].W) / Tb\n",
    "#         loss += 1e-3*sum(abs, cmtlds.nn.layers[2].W) / Tb\n",
    "\n",
    "        epoch_loss += loss.data\n",
    "\n",
    "        # ===== Backprop/update of posterior-integrated likelihood =====\n",
    "        Tracker.back!(loss)\n",
    "\n",
    "        for p in pars\n",
    "#             display(norm(Tracker.grad(p)))\n",
    "            Tracker.update!(opt, p, -Tracker.grad(p))\n",
    "        end\n",
    "\n",
    "    end\n",
    "\n",
    "    history[ee,1] = logp / total_len\n",
    "    history[ee,2] = recon_loss / total_len\n",
    "    if (ee % 1 == 0)\n",
    "        printfmtln(\"Epoch {1:d}, recon loss: {2:.4f}, approx logp(x): {3:.3f}\", ee, history[ee,2], history[ee,1])\n",
    "        flush(stdout)\n",
    "    end\n",
    "\n",
    "end  # for ee in 1:n_epochs\n",
    "model.zero_state!(cmtlds_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history[1:40,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lp = [get_log_px(cmtlds, trainSTL[i][:Y], trainSTL[i][:U], 200) for i in 1:length(trainSTL)]\n",
    "sum(_lp)/sum([size(trainSTL[i][:Y],2) for i in 1:length(trainSTL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _eps = randn(Float32, 3, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printfmtln(\"RMSE = {:.3f}\", rmse(cYs - Yhat)); flush(stdout)\n",
    "# _Z = randn(Float32, 3, 20)\n",
    "dset_i = 4\n",
    "Yhats = [model.make_lds(cmtlds, _eps[1:k,i], cmtlds.η_h[1])(trainSTL[dset_i][:U]) for i in 1:12]\n",
    "\n",
    "[display(p_log_llh(cmtlds, trainSTL[dset_i][:Y], trainSTL[dset_i][:U], _eps[1:k,i])[1]) for i in 1:4];\n",
    "\n",
    "\n",
    "fig, axs = subplots(5,4,figsize=(10,10))\n",
    "offset = 0\n",
    "offset_tt = 400\n",
    "_Δt = 200\n",
    "for i = 1:20\n",
    "    axs[:][i].plot(trainSTL[dset_i][:Y]'[(1:_Δt-1) .+ offset_tt, i+offset])\n",
    "    [axs[:][i].plot(Yhats[j]'[(1:_Δt-1) .+ offset_tt, i+offset], alpha=0.4) for j in 1:12];\n",
    "end\n",
    "# savefig(\"mountains_in_the_sky.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(trainSTL[dset_i][:Y]'[(1:_Δt-1) .+ offset_tt, 2])\n",
    "plot(Yhats[1]'[(1:_Δt-1) .+ offset_tt, 2], alpha=0.8)\n",
    "plot(Yhats[2]'[(1:_Δt-1) .+ offset_tt, 2], alpha=0.8)\n",
    "plot(Yhats[3]'[(1:_Δt-1) .+ offset_tt, 2], alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Z = randn(Float32, 2, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "printfmtln(\"Previously: RMSE/8: {:.3f}, LLH: {:.3f}.\", \n",
    "    rmse(model.make_lds(cmtlds, _Z[:,1], cmtlds.η_h)(validSTL[2][:U]) - validSTL[2][:Y])/8,\n",
    "    p_log_llh(cmtlds, validSTL[1][:Y], validSTL[1][:U], _Z[:,1])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "printfmtln(\"Post-op: RMSE/8: {:.3f}, LLH: {:.3f}.\", \n",
    "    rmse(model.make_lds(cmtlds, _Z[:,1], cmtlds.η_h)(validSTL[2][:U]) - validSTL[2][:Y])/8,\n",
    "    p_log_llh(cmtlds, validSTL[1][:Y], validSTL[1][:U], _Z[:,1])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(trainData, model.make_lds(cmtlds, _Z[:,1], cmtlds.η_h))/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_log_llh(cmtlds_g, first(trainData)[1], first(trainData)[2], _Z[:,1], [1.0f0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.change_relative_lr!(cmtlds, 0.1f0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmtlds_g.η_h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(cmtlds.nn.layers[1].W === cmtlds_g.nn.layers[1].W.data)\n",
    "println(cmtlds.nn.layers[1].b === cmtlds_g.nn.layers[1].b.data)\n",
    "println(cmtlds.nn.layers[2].W === cmtlds_g.nn.layers[2].W.data)\n",
    "println(cmtlds.nn.layers[2].b === cmtlds_g.nn.layers[2].b.data)\n",
    "println(cmtlds.nn.layers[3].W === cmtlds_g.nn.layers[3].W.data)\n",
    "println(cmtlds.nn.layers[3].b === cmtlds_g.nn.layers[3].b.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ψ = cmtlds.nn(_eps[:,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmtlds.logσ\n",
    "precision = 1 ./ exp.(2*cmtlds.logσ)\n",
    "states = Matrix{T}(undef, size(cmtlds,1), size(_eps,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "lds = model._make_lds_psi(cmtlds, Ψ[:,1], cmtlds.η_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X   = model.state_rollout(lds, validSTL[1][:U][:,1:192])\n",
    "Ŷ   = lds.C * X + lds.D * validSTL[1][:U][:,1:192] .+ lds.d;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "-dot(sum(x->x^2, validSTL[1][:Y][:,1:192] - Ŷ, dims=2), precision)/2 -_gauss_lognormconst(cmtlds.logσ, 192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "-dot(sum(x->x^2, validSTL[1][:Y][:,1:192] - Ŷ, dims=2), ones(64))/2 -_gauss_lognormconst(zeros(64), 192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function p_log_llh(mtlds::Union{model.MTLDS_g{T,F}, model.MTLDS_ng{T,F}}, \n",
    "        Y::AbstractMatrix{T}, U::AbstractMatrix{T}, Z::AbstractVecOrMat{T}, \n",
    "        wgt::AbstractVector{T}=ones(T, size(Z, 2))) where {T,F}\n",
    "    Ψ = mtlds.nn(Z)\n",
    "    precision = 1 ./ exp.(2*mtlds.logσ)\n",
    "    obj = zero(T)\n",
    "    states = Matrix{T}(undef, size(mtlds,1), size(Z,2))\n",
    "    \n",
    "    for i in 1:size(Z,2)\n",
    "        lds = model._make_lds_psi(mtlds, Ψ[:,i], mtlds.η_h)\n",
    "        X   = model.state_rollout(lds, U)\n",
    "        Ŷ   = lds.C * X + lds.D * U .+ lds.d;\n",
    "        obj -= wgt[i] * dot(sum(x->x^2, Y - Ŷ, dims=2), precision)/2\n",
    "        states[:,i] = Tracker.data(X)[:,end]\n",
    "    end\n",
    "    return obj, states * (wgt/sum(wgt))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([sum(logpdf.(Normal(0,1), validSTL[1][:Y][i,1:192] - Ŷ[i,:])) for i in 1:64])"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
