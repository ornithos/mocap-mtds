{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mocap Initial Modelling\n",
    "\n",
    "* Deterministic LDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Revise\n",
    "using LinearAlgebra, Random\n",
    "using StatsBase, Statistics\n",
    "using Distributions, MultivariateStats   # Categorical, P(P)CA\n",
    "using Quaternions    # For manipulating 3D Geometry\n",
    "using MeshCat        # For web visualisation / animation\n",
    "using PyPlot         # Plotting\n",
    "using AxUtil         # Cayley, skew matrices\n",
    "using Flux           # Optimisation\n",
    "using DSP            # convolution / low-pass (MA) filter\n",
    "\n",
    "# small utils libraries\n",
    "using ProgressMeter, Formatting, ArgCheck\n",
    "using DelimitedFiles, NPZ, BSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "function zero_grad!(P) \n",
    "    for x in P\n",
    "        x.grad .= 0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_MOCAP_MTDS = \".\" \n",
    "\n",
    "# Data loading and transformation utils\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"io.jl\"))\n",
    "\n",
    "# MeshCat skeleton visualisation tools\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"mocap_viz.jl\"))\n",
    "\n",
    "# Data scaling utils\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"util.jl\"))\n",
    "import .mocaputil: MyStandardScaler, scale_transform, invert\n",
    "import .mocaputil: OutputDifferencer, difference_transform, fit_transform\n",
    "import .mocaputil: no_pos, no_poscp\n",
    "\n",
    "# Models: LDS\n",
    "include(joinpath(DIR_MOCAP_MTDS, \"models.jl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data\n",
    "See `2_Preprocess.ipynb`\n",
    "\n",
    "**Note that in the current harddisk state, `edin_Ys.bson` was created with `include_ftcontact=false`**\n",
    "\n",
    "\n",
    "### Additional changes since `2_Preprocess`:\n",
    "\n",
    "1. Performing a difference transform of joint positions in $Y$. Motivated by trivial predictions of copying previous frame working too well. I want to force the model to learn something.\n",
    "2. Remove joint positions entirely from inputs:\n",
    "    * Don't want prev positions for regression to output ($D$ matrix) as predicting a copy reduces the error close to zero, and makes it difficult to understand which model is performing best for the remaining delta.\n",
    "    * Also don't want in latent state, as positions have already been projected to the latent state (a linear comb) => hence colinear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"../data/mocap/edin-style-transfer/\"\n",
    "files_edin = [joinpath(database, f) for f in readdir(database)];\n",
    "style_name_edin = [x[1] for x in match.(r\"\\.\\./[a-z\\-]+/[a-z\\-]+/[a-z\\-]+/([a-z]+)_.*\", files_edin)];\n",
    "styles = unique(style_name_edin)\n",
    "styles_lkp = [findall(s .== style_name_edin) for s in styles];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Usraw = BSON.load(\"edin_Xs.bson\")[:Xs];\n",
    "Ysraw = BSON.load(\"edin_Ys.bson\")[:Ys];\n",
    "\n",
    "Ys_dtform = [fit_transform(OutputDifferencer, y) for y in Ysraw];\n",
    "Ys, dtforms = [y[2] for y in Ys_dtform], [y[1] for y in Ys_dtform];\n",
    "\n",
    "standardize_Y = fit(MyStandardScaler, reduce(vcat, Ys),  1)\n",
    "standardize_U = fit(MyStandardScaler, reduce(vcat, Usraw),  1)\n",
    "\n",
    "Ys = [scale_transform(standardize_Y, y) for y in Ys];\n",
    "Us = [scale_transform(standardize_U, u[1:end-1,:]) for u in Usraw];  # remove last frame to align with ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_output_tform(y, i) = invert(standardize_Y, y) |> yhat -> invert(dtforms[i], yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can reconstruct the original data via the commands:\n",
    "\n",
    "    invert_output_tform(y, i)       # for (i,y) in enumerate(Ys)\n",
    "    invert(standardize_X, x)        # for x in Xs\n",
    "    \n",
    "in the relevant array comprehensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "?mocapio.construct_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "?mocapio.construct_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(clds_ss(cUT) - cYs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise LDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand PC distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single-task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = subplots(4,2,figsize=(5.5,6))\n",
    "\n",
    "for (i, ixs) in enumerate(styles_lkp)\n",
    "    cYs = reduce(vcat, Ys[ixs])\n",
    "    pc_all = fit(PCA, cYs[:,4:63]', pratio=0.999)\n",
    "\n",
    "    varexpl = cumsum(principalvars(pc_all))/tvar(pc_all)\n",
    "    bd=findfirst.([varexpl .> x for x in [0.9,0.95,0.99]])\n",
    "    axs[:][i].plot(1:length(varexpl), varexpl)\n",
    "    axs[:][i].axhline(1, linestyle=\":\")\n",
    "    for b in bd\n",
    "        axs[:][i].plot([b,b], [varexpl[1], varexpl[b]], color=ColorMap(\"tab10\")(7), linestyle=\":\")\n",
    "        axs[:][i].plot([.5, b], [varexpl[b], varexpl[b]], color=ColorMap(\"tab10\")(7), linestyle=\":\")\n",
    "        axs[:][i].text(b+0.3,varexpl[1]+0.03, b)\n",
    "    end\n",
    "    axs[:][i].set_xlim(0.5,34.5); gca().set_ylim(varexpl[1],1.025);\n",
    "    axs[:][i].xaxis.set_ticklabels([])\n",
    "    axs[:][i].set_title(styles[i])\n",
    "end\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "allE = reduce(vcat, Ys);\n",
    "allE = convert(Matrix{Float32}, allE);\n",
    "\n",
    "pc_all = fit(PCA, allE[:,4:63]', pratio=0.999)\n",
    "\n",
    "varexpl = cumsum(principalvars(pc_all))/tvar(pc_all)\n",
    "bd=findfirst.([varexpl .> x for x in [0.9,0.95,0.99]])\n",
    "plot(1:length(varexpl), varexpl)\n",
    "gca().axhline(1, linestyle=\":\")\n",
    "for b in bd\n",
    "    plot([b,b], [varexpl[1], varexpl[b]], color=ColorMap(\"tab10\")(7), linestyle=\":\")\n",
    "    plot([.5, b], [varexpl[b], varexpl[b]], color=ColorMap(\"tab10\")(7), linestyle=\":\")\n",
    "    text(b+0.3,varexpl[1]+0.03, b)\n",
    "end\n",
    "gca().set_xlim(0.5,37.5); gca().set_ylim(varexpl[1],1.025);\n",
    "gcf().set_size_inches(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDS Initialisation\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{x}_t &= A \\mathbf{x}_{t-1} + B \\mathbf{u}_t + \\mathbf{b}\\\\\n",
    "\\mathbf{y}_t &= C \\mathbf{x}_{t} + D \\mathbf{u}_t + \\mathbf{d}\n",
    "\\end{align}\n",
    "\n",
    "#### Initialisation\n",
    "\n",
    "Note in the below I use the SVD construction for PCA for convenience, but in general $Y$ is not centered ($\\because$ centering is done over \\emph{all} styles simultaneously; each individual will not be centered). Therefore, the below assumes this centering is done temporarily before the SVD.\n",
    "\n",
    "* $C = U_{SVD}$, where $U_{SVD}$ are the prinicipal components of $Y$.\n",
    "* $X = S_{SVD}V_{SVD}^{\\mathsf{T}}$, where $S_{SVD}$, $V_{SVD}$ are the other matrices from the SVD.\n",
    "* $X \\approx \\tilde{U}\\tilde{B} \\Rightarrow \\tilde{B} = (\\tilde{U}^{\\mathsf{T}} \\tilde{U})^{-1} \\tilde{U}^{\\mathsf{T}} X$ (Regression of $X$ on $U$). The permutation of $U$ and $B$ in the first equation follows because the obs are column-wise here, not row-wise.\n",
    "    * Here, $\\tilde{U} = \\begin{bmatrix} U & \\mathbf{1} \\end{bmatrix}$, and hence $\\tilde{B} = \\begin{bmatrix} B & \\mathbf{b} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single task LDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(Δ) = sqrt(mean(x->x^2, Δ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_ix = 1\n",
    "cYs = Matrix(transpose(reduce(vcat, Ys[styles_lkp[style_ix]])))\n",
    "cUs = reduce(vcat, Us[styles_lkp[style_ix]])\n",
    "cUT = Matrix(cUs');\n",
    "cN = size(cYs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple baselines\n",
    "baselines = Dict()\n",
    "baselines[\"copy\"] = rmse(cYs[:,2:end] - cYs[:,1:end-1])\n",
    "\n",
    "cUs_m1 = cUs[1:end-1,:];\n",
    "CDd = cYs[:,2:end] / [cUs_m1'; ones(1, cN-1)]\n",
    "baselines[\"LR\"] = rmse(cYs[:,2:end] - CDd * [cUs_m1'; ones(1, cN-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_init = [rmse(model.init_LDS_spectral(cYs, cUT, k)(cUT) - cYs) for k in 5:5:60];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(5:5:60, perf_init); gcf().set_size_inches(3,2)\n",
    "gca().axhline(baselines[\"copy\"], linestyle=\":\")\n",
    "gca().axhline(baselines[\"LR\"], linestyle=\":\", color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "    \n",
    "clds_g = model.make_grad(model.init_LDS_spectral(cYs, cUT, k))\n",
    "clds   = model.make_nograd(clds_g)   # MUST DO THIS SECOND, (Flux.param takes copy)\n",
    "\n",
    "opt = ADAM(1e-4)\n",
    "opt_hidden = ADAM(0.7e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_hidden = ADAM(0.7e-5)\n",
    "ps_hidden = Flux.params(clds_g.a, clds_g.B, clds_g.b)\n",
    "ps_observ = Flux.params(clds_g.C, clds_g.D, clds_g.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_hidden = Flux.params(clds_g.a, clds_g.B, clds_g.b)\n",
    "ps_observ = Flux.params(clds_g.C, clds_g.D, clds_g.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@time h = begin\n",
    "    n_epochs = 150\n",
    "    history = zeros(n_epochs*58)\n",
    "    N = size(cYs, 2)\n",
    "    \n",
    "    ps = pars(clds_g)\n",
    "\n",
    "#     opt = ADAM(1e-4)\n",
    "    opt.eta = 5e-4 / 5   #/2\n",
    "    opt_hidden.eta = 0.5e-5\n",
    "#     opt.eta *= 2\n",
    "    for ee in 1:n_epochs\n",
    "        if ee % 100 == 0\n",
    "            opt.eta /= 1.5\n",
    "            printfmtln(\"Changed learning rate!\")\n",
    "        end\n",
    "        batch_order = randperm(58)\n",
    "        for tt in batch_order\n",
    "            ixs = (256*(tt-1)+1):min(256*tt, N)\n",
    "            _cY, _cU = cYs[:,ixs[2:end]], cUT[:, ixs[1:end-1]]\n",
    "            Yhat = clds_g(_cU)\n",
    "            obj = mean(x->x^2, _cY - Yhat)\n",
    "            history[(ee-1)*58 + tt] = obj.data\n",
    "            Tracker.back!(obj)\n",
    "            for p in ps_hidden\n",
    "                Tracker.update!(opt_hidden, p, -Tracker.grad(p))\n",
    "            end\n",
    "            for p in ps_observ\n",
    "                Tracker.update!(opt, p, -Tracker.grad(p))\n",
    "            end\n",
    "        end\n",
    "        println(sqrt(mean(history[(1:58) .+ 58*(ee-1)])))\n",
    "    end\n",
    "    history\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(sqrt.(conv(h, Windows.rect(58))[1000:end-57]/58))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the Transition Matrix\n",
    "\n",
    "The transition matrix will be parameterised as:\n",
    "\n",
    "$$ A = D(\\psi_2) Q(\\psi_1) $$\n",
    "\n",
    "where $D$ is a diagonal matrix with elements in $[0,1]$ and $Q$ is a special orthogonal matrix with determinant $+1$. Our goal will be to use the initialisation calculated above, coerced into this form.\n",
    "\n",
    "Unfortunately, we are just as likely to have an initial $A_0$ with determinant $-1$ as $+1$. My previous arguments about measure zero $\\lambda = 1 - 0i$ are not true. It appears that reflections are commonly learned. Therefore, we must deal with this issue later. But first:\n",
    "\n",
    "Decompose:\n",
    "$$ A_0 = U_0 S_0 V_0^T $$\n",
    "\n",
    "Then $D(\\psi_2) = S_0$ and $Q(\\psi_1) = U_0 V_0^T$.\n",
    "\n",
    "\n",
    "### Diagonal Matrix\n",
    "If parameterising the diagonal of $D$ with a sigmoid nonlinearity, we must apply the inverse sigmoid to $\\text{diag}(S)$, i.e. $\\sigma^{-1}(y) = \\ln\\left(\\frac{y}{1-y}\\right)$. Care must be taken to avoid the endpoints $y \\in \\{0, 1\\}$ for numerical reasons, but also because it is not sensible to initialise to a position with no gradient. I have used a minimum distance from the boundaries of $10^{-3}$ (which translates to $x \\approx \\pm 6.9$). \n",
    "\n",
    "\n",
    "### Orthogonal Matrix\n",
    "In order to obtain the Cayley parameterisation of $Q = U_0 V_0^T$ we take the inverse Cayley transformation $S = (I - Q)(I + Q)^{-1}$ to obtain the skew-symmetric matrix $S$ which corresponds 1-to-1 to $Q$. We can then simply extract the lower triangular elements of $S$ as the unique $d(d-1)/2$ elements parameterising $Q$. If these parameters are $\\psi_1$, I will write this as $\\psi_1 = \\text{cayley}^{-1}(Q)$. However, as we have said above, this is insufficient for obtaining a Cayley parameterisation of the estimate $A_0$ in general, since we exclude any $A_0$ s.t. $\\det(A_0) = -1$. Ferrar (1950) tells us that a general orthogonal matrix can be parameterised as $J(I-S)(I+S)^{-1}$, where $J$ is a diagonal matrix with elements in $\\{+1, -1\\}$. Crucially we need as many negative elements ($-1$) as their are negative roots of $Q_0$ and we may choose them for convenience to precede all of the positive elements in $J$.\n",
    "\n",
    "#### Corollary\n",
    "\n",
    "1. $ A = D(\\psi_2) Q(\\psi_1) $ as before, but now with $D$ containing elements in $[-1,1]$ is sufficient to parameterise *any* orthogonal matrix. Note that we may instead use $\\tanh$ instead of $\\sigma$ to achieve this.\n",
    "2. For the problem at hand, we need a *special* orthogonal matrix in order to apply the inverse Cayley transform. Now we know that $Q$ can be represented as \n",
    "    $$Q = J\\tilde{Q}$$\n",
    "    for $\\tilde{Q} \\in SO(d)$. Then clearly $\\tilde{Q} = JQ$ and hence we have that \n",
    "    $$Q = J\\,\\text{cayley}\\left(\\text{cayley}^{-1}(JQ)\\right).$$\n",
    "\n",
    "Therefore we can parameterise $A_0 = J\\,D(\\psi_2) Q(\\psi_1)$ where $\\psi_1 =  \\text{cayley}^{-1}(JQ)$ where $J$ is a $\\{+1,-1\\}$ diagonal calculated directly from the eigenvalues of $Q=U_0 V_0^T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = clds(cUT);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "printfmtln(\"RMSE = {:.3f}\", rmse(cYs - Yhat)); flush(stdout)\n",
    "\n",
    "fig, axs = subplots(5,4,figsize=(10,10))\n",
    "offset = 20\n",
    "offset_tt = 0\n",
    "for i = 1:20\n",
    "    axs[:][i].plot(Ys[1][(1:100) .+ offset_tt, i+offset])\n",
    "    axs[:][i].plot(Yhat'[(1:100) .+ offset_tt, i+offset])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "?mocapio.construct_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "function unsup_predict(lds::model.MyLDS_ng{T}, U::AbstractMatrix{T}, \n",
    "        Yraw5::AbstractVector{T}, YsTrue::AbstractMatrix{T}, standardize_Y::MyStandardScaler,\n",
    "        standardize_U::MyStandardScaler) where T <: AbstractFloat\n",
    "    \n",
    "    n = size(U, 2)\n",
    "    A = model.Astable(lds)\n",
    "\n",
    "    X = Matrix{T}(undef, size(lds, 1), n);\n",
    "    Y = Matrix{T}(undef, size(lds, 2), n)\n",
    "    u = Vector{T}(undef, size(lds, 3))\n",
    "    \n",
    "    # transform Y -> U\n",
    "    cy   = Yraw5\n",
    "    μ, σ = standardize_U.μ[61:121], standardize_U.σ[61:121]\n",
    "    \n",
    "    X[:,1] = A*lds.h + lds.B*U[:, 1] + lds.b\n",
    "    Y[:,1] = lds.C * X[:,1] + lds.D * U[:,1] .+ lds.d\n",
    "    for i in 2:n\n",
    "        u = U[:,i]   # I found some unexpected behaviour when using views here.\n",
    "        if i > 5\n",
    "            y_unnorm = invert(standardize_Y, reshape(YsTrue[:,(i-1)], 1, 64)) |> vec\n",
    "            cy = y_unnorm + cy  # (cum)sum here inverts differencing.\n",
    "#             display( hcat((cy[4:64] - μ) ./ σ, u[61:121]))\n",
    "            u[61:121] = (cy[4:64] - μ) ./ σ            # transform to u space\n",
    "        end\n",
    "        @views X[:,i] = A*X[:,i-1] + lds.B*u + lds.b\n",
    "        @views Y[:,i] = lds.C * X[:,i] + lds.D * u .+ lds.d\n",
    "    end\n",
    "    return Y\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ysraw[1][1:5,4:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dtform = fit(OutputDifferencer, Ysraw[1][1:2,:])\n",
    "tmp = invert(standardize_Y, Matrix(cYs[:,1:5]')) |> yhat -> invert(_dtform, yhat)\n",
    "tmp[:,4:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert(standardize_U, Matrix(cUT[:,1:5]'))[:,61:121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function invert_from_frame(file_ix, start_frame, y)\n",
    "    _dtform = fit(OutputDifferencer, Ysraw[file_ix][start_frame:start_frame+1,:])\n",
    "    invert(standardize_Y, y) |> yhat -> invert(_dtform, yhat)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tmp[:,4:64] .- standardize_U.μ[61:121]') ./ standardize_U.σ[61:121]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(unsup_predict(clds, cUT[:,1:100], Ysraw[1][5,:], cYs[:,1:100], standardize_Y, standardize_U) - cYs[:,1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(clds(cUT[:,1:100]) - cYs[:,1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "cUT[61:121,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(unsup_predict(clds, cUT[:,1:20], Ysraw[1][1,:], standardize_Y, standardize_U)[1,1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "function invert_from_frame(file_ix, start_frame, y)\n",
    "    _dtform = fit(OutputDifferencer, Ysraw[file_ix][start_frame:start_frame+1,:])\n",
    "    invert(standardize_Y, y) |> yhat -> invert(_dtform, yhat)\n",
    "end\n",
    "Yhat_r = begin; ifr=501; Yr=Matrix(clds(cUT[:,ifr:ifr+1000])'); invert_from_frame(1, ifr, Yr); end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "if !(@isdefined vis) \n",
    "    # Create a new visualizer instance (MeshCat.jl)\n",
    "    vis = Visualizer()\n",
    "    open(vis)\n",
    "end\n",
    "vis = mocapviz.create_animation([mocapio.reconstruct_modelled(Yhat_r)[1:500,:,:], \n",
    "                                 mocapio.reconstruct_modelled(Ysraw[1][501:1000,:])], \n",
    "    \"test\"; vis=vis, linemesh=[mocapviz.redmesh, mocapviz.yellowmesh], camera=:back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are RNNs easier to train?\n",
    "\n",
    "(** Answer: No... not by a long shot!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = size(cYs, 1)\n",
    "d_in = size(cUT, 1)\n",
    "d_state = 30\n",
    "d_ff = 50\n",
    "diffdmodel = 0  #63\n",
    "\n",
    "rnn = RNN(d_in, d_state, elu)\n",
    "ffnn = Chain(Dense(d_state, d_ff), Dense(d_ff, d_out, identity))\n",
    "\n",
    "a = param(zeros(Float32, Int(d_state*(d_state-1)/2)))\n",
    "cUs_m1 = cUs[1:end-1,:];\n",
    "CDd = cYs[:,2:end] / [cUs_m1'; ones(1, cN-1)]\n",
    "D, C = Flux.param(deepcopy(CDd[:,1:end-1])), param(Flux.glorot_uniform(d_out, d_state))\n",
    "ffnn.layers[2].b.data .= CDd[:,end];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = ADAM(1e-3)\n",
    "pars = Flux.params(rnn, ffnn.layers[1], ffnn.layers[2], C, D, a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_grad!(pars)\n",
    "Flux.reset!(rnn)\n",
    "Flux.truncate!(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = ADAM(1e-7)\n",
    "pars = Flux.params(rnn.cell.Wi, rnn.cell.b, ffnn, C, D, a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "?mocapio.construct_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "lds_cell.B.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "history = zeros(Float32, n_epochs)\n",
    "N = size(cYs, 2)\n",
    "cUList = [cUT[:,i] for i in 1:N]\n",
    "\n",
    "opt.eta = 2e-3 / 40\n",
    "for ee in 1:n_epochs\n",
    "    batch_order = randperm(58)\n",
    "    for tt in batch_order\n",
    "        ixs = (256*(tt-1)+1):min(256*tt, N)\n",
    "        _cY, _cU, _cUL = cYs[:,ixs[2:end]], cUT[:, ixs[1:end-1]], cUList[ixs[1:end-1]]\n",
    "        rnn.cell.Wh = AxUtil.Math.cayley_orthog(a/10, d_state)\n",
    "        x̂ = rnn.(_cUL)\n",
    "        x̂ = Tracker.collect(reduce(hcat, x̂))\n",
    "        ŷ = ffnn(x̂) + C*x̂ + D*_cU\n",
    "        obj = mean(x->x^2, _cY - ŷ)\n",
    "        Tracker.back!(obj)\n",
    "        history[ee] += obj.data / length(batch_order)\n",
    "        for p in pars\n",
    "            Tracker.update!(opt, p, -Tracker.grad(p))\n",
    "        end\n",
    "        Flux.reset!(rnn)\n",
    "        Flux.truncate!(rnn)\n",
    "    end\n",
    "    println(sqrt(history[ee]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ee in 1:1\n",
    "    h = 0\n",
    "    batch_order = randperm(58)\n",
    "    for tt in batch_order\n",
    "        ixs = (256*(tt-1)+1):min(256*tt, N)\n",
    "        _cY, _cU, _cUL = cYs[:,ixs[2:end]], cUT[:, ixs[1:end-1]], cUList[ixs[1:end-1]]\n",
    "        ŷ = D*_cU .+ ffnn.layers[2].b\n",
    "        obj = mean(x->x^2, _cY - ŷ)\n",
    "        h += obj.data\n",
    "    end\n",
    "    println(sqrt(h/length(batch_order)))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
